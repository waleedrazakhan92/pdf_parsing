{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNlHxjQU18Ml",
        "outputId": "e78d7424-9473-4c47-c533-ff00b67064ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r '/content/pdf_parsing/'"
      ],
      "metadata": {
        "id": "EwFhiEqE19o3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/waleedrazakhan92/pdf_parsing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyQCar_w0ql5",
        "outputId": "a9ba3d36-e167-43dd-eeaa-f48002718859"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pdf_parsing'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/pdf_parsing/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEeXM_3K0urc",
        "outputId": "078a7021-74f5-4d66-bf07-6632e75b6932"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pdf_parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir utils"
      ],
      "metadata": {
        "id": "gtHtcg2800Yp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/pdf_parsing/utils/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ktvQQ7703nd",
        "outputId": "52bd83ca-2e83-4a45-a27c-50b482ed0f2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pdf_parsing/utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAwD4WZgfLPa",
        "outputId": "ba917d20-971a-4a1e-8492-21a6d455368d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "llrXKK98fMhK"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/drive/MyDrive/shared_with_faizan/micheal_solomon/documents/' '/content/'\n",
        "# !cp -r '/content/drive/MyDrive/shared_with_faizan/micheal_solomon/junk/page_tables/' '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lhSfiPXfN8t"
      },
      "outputs": [],
      "source": [
        "!apt-get install tesseract-ocr\n",
        "!apt-get install libtesseract-dev\n",
        "!pip install pytesseract\n",
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install deskew\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/pdf_parsing/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGMbXAov8sTh",
        "outputId": "566f0f69-2027-4fd8-a1cf-bb4633de90bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pdf_parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KergP3fm9QTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1vtJk4svpX3"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rPtfBIaJ70x"
      },
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTIu8Hm_J-Pa",
        "outputId": "1fec9652-99b4-413a-d578-43597a82f256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting load_and_preprocess_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile load_and_preprocess_utils.py\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from PIL import Image, ImageChops\n",
        "\n",
        "# for de skew\n",
        "from deskew import determine_skew  ## gray image\n",
        "import numpy as np\n",
        "from skimage.transform import rotate\n",
        "\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# def cv2_imshow_rgb(img,resize=None):\n",
        "#     if resize!=None:\n",
        "#         img=cv2.resize(img,resize)\n",
        "\n",
        "\n",
        "#     cv2_imshow(cv2.cvtColor(np.uint8(img),cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def cv2_imshow_rgb(img,resize=None, figsize=(15,15)):\n",
        "    if resize!=None:\n",
        "        img=cv2.resize(img,resize)\n",
        "\n",
        "        #     cv2_imshow(cv2.cvtColor(np.uint8(img),cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(np.uint8(img))\n",
        "\n",
        "\n",
        "def display_multi(*images,resize=None, figsize=(15,15),bgr=False,axis=1):\n",
        "    if resize!=None:\n",
        "        res = np.array(cv2.resize(images[0],resize))\n",
        "    else:\n",
        "        res = np.array(images[0])\n",
        "\n",
        "    for i in range(1,len(images)):\n",
        "\n",
        "        if resize!=None:\n",
        "            res_img = np.array(cv2.resize(images[i],resize))\n",
        "        else:\n",
        "            res_img = np.array(images[i])\n",
        "\n",
        "        res = np.concatenate((res, res_img), axis=axis)\n",
        "\n",
        "    if bgr==True:\n",
        "        res = cv2.cvtColor(res,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return cv2_imshow_rgb(res,resize=None, figsize=figsize)\n",
        "\n",
        "def load_img(img_path, rgb=True, size=False):\n",
        "    img = cv2.imread(img_path)\n",
        "    if rgb==True:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    if size:\n",
        "        img = cv2.resize(img, size)\n",
        "\n",
        "    return img\n",
        "\n",
        "def write_text_file(text_file_path,output_text,remove_empty_lines=True):\n",
        "    if remove_empty_lines==True:\n",
        "        output_text = output_text.split('\\n')\n",
        "\n",
        "    # Save the extracted text to a file\n",
        "    with open(text_file_path, 'w') as f:\n",
        "        if remove_empty_lines==True:\n",
        "            for line in output_text:\n",
        "                if line.strip():\n",
        "                    f.write(\"%s\\n\" % line)\n",
        "        else:\n",
        "            f.write(output_text)\n",
        "\n",
        "\n",
        "def deskew_image(image,cval=1.0):\n",
        "    ## https://pypi.org/project/deskew/#:~:text=Deskewing%20is%20a%20process%20whereby,rather%20than%20at%20an%20angle.\n",
        "    angle = determine_skew(image)\n",
        "    image = rotate(image, angle, resize=True,cval=cval) * 255\n",
        "    image = image.astype(np.uint8)\n",
        "    return image\n",
        "\n",
        "\n",
        "def add_image_border(img,border_width,border_color):\n",
        "    img_border = cv2.copyMakeBorder(img,border_width,border_width,border_width,border_width,\n",
        "                                    cv2.BORDER_CONSTANT,value=border_color)\n",
        "    return img_border\n",
        "\n",
        "\n",
        "def save_all_pages_raw(temp_path,all_pages,dpi=(300,300)):\n",
        "    if all_pages:\n",
        "        os.makedirs(temp_path, exist_ok=True)\n",
        "\n",
        "        # Save images to the temporary directory\n",
        "        for i in tqdm(range(0,len(all_pages))):\n",
        "            image = all_pages[i]\n",
        "\n",
        "            image_path = os.path.join(temp_path,\"page_\"+str(i+1)+\".tiff\")\n",
        "            # image = Image.fromarray(image)\n",
        "            image.save(image_path,dpi=dpi)\n",
        "            image.save(image_path.split('.tiff')[0]+'.png',dpi=dpi)\n",
        "\n",
        "\n",
        "def preprocess_image(image,deskew=True,img_thresh=False,add_border=False,border_width=10,border_color=(128,128,128),img_type='rgb'):\n",
        "    assert img_type=='rgb' or img_type=='gray'\n",
        "\n",
        "    # expects rgb image\n",
        "    if img_type=='rgb':\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    if deskew==True:\n",
        "        image = deskew_image(image,cval=1.0)\n",
        "\n",
        "    if add_border==True:\n",
        "        image = add_image_border(image,border_width,border_color)\n",
        "\n",
        "    if img_thresh==True:\n",
        "        ret,image = cv2.threshold(image,220,255,cv2.THRESH_BINARY)\n",
        "\n",
        "    if img_type=='rgb':\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "## rotate using tesseract\n",
        "def check_orientation_tesseract(img):\n",
        "    newdata = pytesseract.image_to_osd(np.array(img))\n",
        "    orgi_orientation = re.search('(?<=Rotate: )\\d+', newdata).group(0)\n",
        "    # check image orientation\n",
        "    if orgi_orientation != 0:\n",
        "        return rotate_image_tesseract(img)\n",
        "    else:\n",
        "        return img\n",
        "\n",
        "## pip install opevcv-python==4.1.0.25 (this functionality works only in v4.1.0.25 )\n",
        "def rotate_image_tesseract(image, center=None, scale=1.0):\n",
        "    angle = 360 - int(re.search('(?<=Rotate: )\\d+', pytesseract.image_to_osd(image)).group(0))\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    if center is None:\n",
        "        center = (w / 2, h / 2)\n",
        "\n",
        "    # Perform the rotation\n",
        "    mmm = cv2.getRotationMatrix2D(center, angle, scale)\n",
        "    rotated = cv2.warpAffine(image, mmm, (w, h))\n",
        "\n",
        "    return rotated\n",
        "\n",
        "\n",
        "def trim_image_borders(im,margin=10):\n",
        "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
        "    diff = ImageChops.difference(im, bg)\n",
        "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
        "    bbox = diff.getbbox()\n",
        "\n",
        "    if bbox:\n",
        "        x,y,x_max,y_max = bbox\n",
        "        x = max(x-margin,0)\n",
        "        y = max(y-margin,0)\n",
        "        x_max = min(x_max+margin,im.size[0]-1)\n",
        "        y_max = min(y_max+margin,im.size[1]-1)\n",
        "        bbox = (x,y,x_max,y_max)\n",
        "        return im.crop(bbox)\n",
        "    else:\n",
        "        return im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pVhgkbH9OV7"
      },
      "source": [
        "## All Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmmiBioUJ11-",
        "outputId": "33d69dbc-473b-4fdf-8118-d28f9114e065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting notice_and_case_types.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile notice_and_case_types.py\n",
        "##############################################################\n",
        "## Notice types( from Tables)\n",
        "##############################################################\n",
        "\n",
        "notice_types_tables = {}\n",
        "## same lines i.e \"Notice Type: XYZ\"\n",
        "notice_types_tables['Reopen'] = 'Reopen Notice'\n",
        "notice_types_tables['Transfer'] = 'Transfer Notice'\n",
        "notice_types_tables['USCIS'] = 'USCIS Account Access'\n",
        "notice_types_tables['Approval'] = 'Approval Notice'\n",
        "## Next lines i.e:\n",
        "## Notice Type\n",
        "## Receipt\n",
        "notice_types_tables['Receipt'] = 'Receipt'\n",
        "notice_types_tables['Rejection'] = 'Rejection Notice'\n",
        "\n",
        "'''\n",
        "Receipt with no \"notice type\" heading \"same as ASC Appointment\"  same as \"Initial interview\"\n",
        "'''\n",
        "notice_types_tables['Interview'] = 'Request for Applicant to Appear for '\n",
        "notice_types_tables['ASC'] = 'ASC Appointment Notice'\n",
        "\n",
        "# notice_types_tables['Receipt_2'] = 'Receipt'\n",
        "notice_types_tables['Receipt_2'] = 'Recei' ## if it was cut by line\n",
        "\n",
        "notice_types_tables['Interview_2'] = 'Please come to:'\n",
        "notice_types_tables['Cancellation'] = 'Notice of Interview Cancellation'\n",
        "notice_types_tables['Applicants'] = 'Notice to Applicants'\n",
        "notice_types_tables['Biometric'] = 'Biometric'\n",
        "\n",
        "\n",
        "##############################################################\n",
        "## Document types\n",
        "##############################################################\n",
        "all_doc_tags = {}\n",
        "all_doc_tags['Courtesy_letter'] = 'COURTESY LETTER TO APPLICANT'\n",
        "all_doc_tags['NIVCC'] = 'NOTICE OF IMMIGRANT VISA CASE CREATION'\n",
        "all_doc_tags['Withdrawal_Acknowledgment'] = 'ACKNOWLEDGMENT OF WITHDRAWAL'\n",
        "all_doc_tags['Withdrawal'] = 'WITHDRAWAL'\n",
        "all_doc_tags['Decision'] = 'DECISION'\n",
        "all_doc_tags['Decision_Notice'] = 'NOTICE OF DECISION'\n",
        "all_doc_tags['RFE'] = 'REQUEST FOR EVIDENCE'\n",
        "# all_doc_tags['RFE_2'] = 'REQUEST FOR EVIDENCE (FORM I-485)'\n",
        "# all_doc_tags['RFE_3'] = 'REQUEST FOR EVIDENCE (FORM 1-485)'\n",
        "all_doc_tags['Deficiency_Notice'] = 'DEFICIENCY NOTICE'\n",
        "# all_doc_tags['DefN_2'] = 'I-693 DEFICIENCY NOTICE'\n",
        "# all_doc_tags['DefN_3'] = '1-693 DEFICIENCY NOTICE'\n",
        "all_doc_tags['Oath_Ceremony'] = 'NOTICE OF NATURALIZATION'\n",
        "\n",
        "## Partial Tags\n",
        "all_doc_tags_partial = ['REQUEST FOR EVIDENCE','DEFICIENCY NOTICE','NOTICE OF NATURALIZATION']\n",
        "\n",
        "##############################################################\n",
        "## Case types\n",
        "##############################################################\n",
        "# ['I-129F','I-130','I-485','I-131','I-765','N-400','I-751','I-90','I-539']\n",
        "all_case_types_org = ['-129F','-130','-485','-131','-765','-400','-751','-539','-290B','-90']\n",
        "all_case_types_tables = ['129F','130','485','131','765','400','751','539','290B','90']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table Utils"
      ],
      "metadata": {
        "id": "r4p63V3D6KuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s01vhV_G7wQQ",
        "outputId": "362b1b86-bb27-4d26-e035-b31563b44823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting table_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile table_utils.py\n",
        "\n",
        "from utils.common_functions import *\n",
        "from utils.load_and_preprocess_utils import add_image_border\n",
        "import pytesseract\n",
        "import os\n",
        "import cv2\n",
        "#from pytesseract import Output\n",
        "import numpy as np\n",
        "import skimage\n",
        "from utils.load_and_preprocess_utils import display_multi\n",
        "\n",
        "def apply_morph(img,morph_operation=cv2.MORPH_CLOSE,kernel=(3,3),iterations=1):\n",
        "    img = cv2.morphologyEx(img, morph_operation, kernel=kernel,iterations=iterations)\n",
        "    return img\n",
        "\n",
        "## case type in case of tables\n",
        "def find_case_type_from_tables(extracted_text,all_case_types):\n",
        "    '''\n",
        "    finds tag lines in extracted text\n",
        "    '''\n",
        "    det_case_type = None\n",
        "    for c in ['Case Type','CaseType']:\n",
        "        c_t_idx = find_element_index(extracted_text,c)\n",
        "        if c_t_idx!=[]:\n",
        "            break\n",
        "\n",
        "\n",
        "    if c_t_idx!=[] and len(extracted_text)>1:\n",
        "        tag_line = extracted_text[c_t_idx+1]\n",
        "        for in_tag in all_case_types:\n",
        "            if in_tag in tag_line:\n",
        "                return in_tag,tag_line\n",
        "        else:\n",
        "            return None,None\n",
        "    else:\n",
        "        return None,None\n",
        "\n",
        "\n",
        "\n",
        "def test_cropped_to_text(in_crop):\n",
        "    extracted_text = pytesseract.image_to_string(in_crop)\n",
        "    print(extracted_text)\n",
        "    if extracted_text != '\\x0c':\n",
        "        extracted_text = delete_empty_lines(extracted_text)\n",
        "\n",
        "    return extracted_text\n",
        "    # print('----------------------')\n",
        "    # print(extracted_text)\n",
        "\n",
        "def draw_fake_line(img,line_hei=0.5,line_thick=4):\n",
        "    '''\n",
        "    drawing a fake line in the middle of the document to\n",
        "    cater for entries which didnt come inside a table at first i.e name, c/o, etc\n",
        "    '''\n",
        "    return cv2.line(img, (0,int(line_hei*img.shape[0])), (img.shape[1],int(line_hei*img.shape[0])), 0, line_thick)\n",
        "\n",
        "\n",
        "def make_fake_table(img,table_coordinates,h_limit=0.35,w_limit=0.6):\n",
        "    '''\n",
        "    pre_reqs = other tables were found\n",
        "    generate a fake table by finding the maximum y from old tables\n",
        "    then from y_max to img_height*h_limit and 0:w_limit crops a fake table\n",
        "    '''\n",
        "    img_h,img_w = img.shape\n",
        "    coordis_array = np.array(table_coordinates['cropped'])\n",
        "    max_y_idx = np.argmax(coordis_array[:,5]) ## format = (x,y,w,h,x+w,y+h)\n",
        "    max_y = coordis_array[:,5][max_y_idx]\n",
        "    fake_table = img[max_y:int(img_h*h_limit), :int(img_w*w_limit)]\n",
        "\n",
        "    ## appending fake_table_coordinates to table_coordinates too\n",
        "    x,y,w,h = 0, max_y, int(img_w*w_limit), int(img_h*h_limit)-max_y\n",
        "    table_coordinates['cropped'].append((x,y,w,h,x+w,y+h))\n",
        "\n",
        "    return fake_table,table_coordinates\n",
        "\n",
        "\n",
        "## Notice type\n",
        "def find_notice_tags(all_tags,extracted_text):\n",
        "    '''\n",
        "    finds tag lines in extracted text\n",
        "    '''\n",
        "    det_notice_type = None\n",
        "    for nt_tag in ['Notice Type','NoticeType']:\n",
        "        n_t_idx = find_element_index(extracted_text,nt_tag)\n",
        "        if n_t_idx!=[]:\n",
        "            break\n",
        "\n",
        "    if n_t_idx!=[]:\n",
        "        if len(extracted_text)==1:\n",
        "            for tag in list(all_tags.values()):\n",
        "                if tag.lower() in extracted_text[n_t_idx].lower():\n",
        "                    det_notice_type = tag\n",
        "                    break\n",
        "        elif len(extracted_text)>1:\n",
        "            for tag in list(all_tags.values()):\n",
        "                for tag_index in range(n_t_idx,len(extracted_text)):\n",
        "                    if tag.lower() in extracted_text[tag_index].lower():\n",
        "                        det_notice_type = tag\n",
        "                        break\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    else:\n",
        "        if len(extracted_text)==1:\n",
        "            for tag in list(all_tags.values()):\n",
        "                det_notice_idx = find_element_index(extracted_text,tag)\n",
        "                if det_notice_idx!=[]:\n",
        "                    det_notice_type = tag\n",
        "                    break\n",
        "        else:\n",
        "            for tag in ['Please come to:']:#list(all_tags.values()):\n",
        "                '''\n",
        "                this is just to cater \"please come to:\" forms this phrase appear\n",
        "                in the top line of the big box\n",
        "                '''\n",
        "                det_notice_idx = find_element_index(extracted_text,tag)\n",
        "                if det_notice_idx!=[] and det_notice_idx==0:\n",
        "                    det_notice_type = tag\n",
        "                    break\n",
        "\n",
        "    if det_notice_type!=None:\n",
        "        return find_key_by_val(all_tags,det_notice_type)\n",
        "    else:\n",
        "        return det_notice_type\n",
        "\n",
        "def extract_all_tables(img,sp_name,path_cropped,filter_tables=True,save_crops=False,save_overlay=True,save_ext='.jpg',fake_table=True,fake_line=False,resize=2):\n",
        "    ## https://github.com/arnavdutta/Table-Detection-Extraction/tree/master\n",
        "\n",
        "    if save_crops==True or save_overlay==True:\n",
        "        if not os.path.isdir(path_cropped):\n",
        "            os.mkdir(path_cropped)\n",
        "\n",
        "    ## Resizing to make text predictions better\n",
        "    if resize!=None:\n",
        "        in_img_h,in_img_w = img.shape\n",
        "        img = cv2.resize(img,(int(in_img_w*resize),int(in_img_h*resize)))\n",
        "\n",
        "\n",
        "    img_gray = img.copy()\n",
        "    im_h,im_w = img_gray.shape\n",
        "\n",
        "    if fake_line==True:\n",
        "        img_gray = draw_fake_line(img_gray.copy(),line_hei=0.4,line_thick=4)\n",
        "\n",
        "\n",
        "    ret,img_bin = cv2.threshold(img_gray,220,255,cv2.THRESH_BINARY)\n",
        "    img_bin = cv2.bitwise_not(img_bin)\n",
        "\n",
        "    kernel_length_v = (np.array(img_gray).shape[1])//120\n",
        "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, kernel_length_v))\n",
        "    im_temp1 = cv2.erode(img_bin, vertical_kernel, iterations=2)#3\n",
        "    vertical_lines_img = cv2.dilate(im_temp1, vertical_kernel, iterations=3)\n",
        "\n",
        "    kernel_length_h = (np.array(img_gray).shape[1])//40\n",
        "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_length_h, 1))\n",
        "    im_temp2 = cv2.erode(img_bin, horizontal_kernel, iterations=2)#3\n",
        "    horizontal_lines_img = cv2.dilate(im_temp2, horizontal_kernel, iterations=3)\n",
        "\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "    table_segment = cv2.addWeighted(vertical_lines_img, 0.5, horizontal_lines_img, 0.5, 0.0)\n",
        "    table_segment = cv2.erode(cv2.bitwise_not(table_segment), kernel, iterations=2)\n",
        "    thresh, table_segment = cv2.threshold(table_segment, 0, 255, cv2.THRESH_OTSU)\n",
        "\n",
        "    contours, hierarchy = cv2.findContours(table_segment, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    overlayed_tables = []\n",
        "    all_tables = {}\n",
        "    all_tables['cropped'] = []\n",
        "    table_coordinates = {}\n",
        "    table_coordinates['cropped'] = []\n",
        "\n",
        "    count = 0\n",
        "    for c in contours:\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "        cropped = img_gray[y:y+h, x:x+w]\n",
        "        # if (w>80 and h>20) and w>3*h:\n",
        "        if filter_tables==True: ## in case of resize==2\n",
        "            if (w>50 and h>20) and (y+h)<(im_h*0.5) and y>(im_h*0.05) and len(np.unique(cropped))!=1 and (w>h):\n",
        "                count += 1\n",
        "                cv2.rectangle(img,(x, y),(x + w, y + h),(0, 255, 0), 2)\n",
        "                all_tables['cropped'].append(cropped)\n",
        "                table_coordinates['cropped'].append((x, y, w,h, x+w,y+h))\n",
        "\n",
        "                if save_crops==True:\n",
        "                    path_crp = os.path.join(path_cropped,sp_name+'_'+'crop_'+str(count)+save_ext)\n",
        "                    cv2.imwrite(path_crp, cropped)\n",
        "        else:\n",
        "            count += 1\n",
        "            cv2.rectangle(img,(x, y),(x + w, y + h),(0, 255, 0), 2)\n",
        "            all_tables['cropped'].append(cropped)\n",
        "            table_coordinates['cropped'].append((x, y, w,h, x+w,y+h))\n",
        "\n",
        "            if save_crops==True:\n",
        "                path_crp = os.path.join(path_cropped,sp_name+'_'+'crop_'+str(count)+save_ext)\n",
        "                cv2.imwrite(path_crp, cropped)\n",
        "\n",
        "    if save_overlay==True:\n",
        "        path_overlay = os.path.join(path_cropped,sp_name+'_bb'+save_ext)\n",
        "        cv2.imwrite(path_overlay, img)\n",
        "        # cv2.imwrite(\"/content/tb/table_detect_\" + sp_name+save_ext, table_segment)\n",
        "\n",
        "    all_tables['table_segment'] = table_segment\n",
        "    all_tables['table_overlayed'] = img\n",
        "\n",
        "    if fake_table==True:\n",
        "\n",
        "        all_tables_fake = all_tables.copy()\n",
        "        table_coordinates_fake = table_coordinates.copy()\n",
        "\n",
        "        ## fake table\n",
        "        if table_coordinates['cropped']!=[]:\n",
        "            fake_table,table_coordinates_fake = make_fake_table(img,table_coordinates_fake,h_limit=0.35,w_limit=0.6)\n",
        "            all_tables_fake['cropped'].append(fake_table)\n",
        "            '''Returns fake table with fake coordinates '''\n",
        "            return all_tables_fake,table_coordinates_fake\n",
        "    '''Else return original table with original coordinates '''\n",
        "    return all_tables,table_coordinates\n",
        "\n",
        "\n",
        "def cropped_tables_to_data(all_tables,table_coordinates,fake_table,notice_types_tables,all_case_types,\n",
        "                           thresh_filter=False,op_borders=None,display_info=False,op_morph=None,narrow_cell=True):\n",
        "    name_co = None\n",
        "    co = None\n",
        "    # case_type = None\n",
        "    case_type_tables = None\n",
        "    notice_type = None\n",
        "\n",
        "    applicant = None\n",
        "    petitioner = None\n",
        "    beneficiary = None\n",
        "    page_num = None\n",
        "    for cr in range(0,len(all_tables['cropped'])):\n",
        "        '''Fake table condition is to limit the finding of only co and name_co in the fake table. As only name and name_co exist in that.\n",
        "        For everything else do not look into the fake table. It also assumes that fake table is appended at the last index.\n",
        "        So if fake_table==False look in all the indexes but if its true, do not look into it exept for name and name_co'''\n",
        "        fake_table_condition = (fake_table==True and cr<len(all_tables['cropped'])-1) or (fake_table==False)\n",
        "        cropped_img = all_tables['cropped'][cr]\n",
        "\n",
        "        if min(cropped_img.shape)==0:\n",
        "            continue\n",
        "\n",
        "        if narrow_cell==True:\n",
        "            cropped_contours,eroded_img = erode_and_find_contours(cropped_img,erode_iters=10)\n",
        "            if display_info==True:\n",
        "                display_multi(eroded_img)\n",
        "\n",
        "            if len(cropped_contours)!=0:\n",
        "                cropped_img = cropped_contours[0]\n",
        "\n",
        "\n",
        "        crp_h,crp_w = cropped_img.shape\n",
        "\n",
        "        if op_borders!=None:\n",
        "            ## add border\n",
        "            cropped_img = add_image_border(np.array(cropped_img),op_borders,(255,255,255))  ## adding white border to make text not so narroly cropped\n",
        "\n",
        "        ## testing simple vs otsu thresholding\n",
        "        # cropped_img = cv2.threshold(cropped_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "        if thresh_filter!=None:\n",
        "            cropped_img = skimage.util.img_as_ubyte(cropped_img > thresh_filter(cropped_img))\n",
        "\n",
        "        if op_morph!=None:\n",
        "            cropped_img = op_morph(cropped_img)\n",
        "\n",
        "        ###extracted_text = pytesseract.image_to_string(cropped_img)\n",
        "        data_dict = pytesseract.image_to_data(cropped_img, output_type=pytesseract.Output.DICT)\n",
        "        data_dict = clean_the_data_dict(data_dict,del_list=['',' '])\n",
        "        extracted_text,all_lines,all_bboxes = text_to_old_format(data_dict)\n",
        "\n",
        "        if display_info==True:\n",
        "            print(extracted_text)\n",
        "            display_multi(cropped_img)\n",
        "\n",
        "\n",
        "        if extracted_text != '\\x0c':\n",
        "            ##extracted_text = delete_empty_lines(extracted_text)\n",
        "\n",
        "            if co==None and name_co==None:\n",
        "                ## for searching c/o and name_co\n",
        "                cropped_img_thresh,extracted_text_thresh,all_lines_thresh,all_bboxes_thresh = try_with_thresholding(cropped_img)\n",
        "                co,name_co = find_name_co_v2(extracted_text_thresh,all_lines_thresh,all_bboxes_thresh,dist_limit=30)\n",
        "                if display_info==True:\n",
        "                    print(extracted_text_thresh)\n",
        "                    display_multi(cropped_img_thresh)\n",
        "\n",
        "\n",
        "            if applicant==None and fake_table_condition==True:\n",
        "                '''\n",
        "                think of limiting the len(extracted_text)<=3\n",
        "                because in some cases there appears \"Applicant Name/Email address\"\n",
        "                which you dont want to detect\n",
        "                in some cases its Applicant then next line name,\n",
        "                in other case its Applicant then next line some junk id then in third line some name\n",
        "                '''\n",
        "\n",
        "                for ap_tag in ['applicant','applica']:\n",
        "                    indices = find_element_index(extracted_text,ap_tag)\n",
        "                    if indices!=[] and ('Application'.lower() not in extracted_text[indices].lower()):\n",
        "                        applicant = extracted_text[-1]\n",
        "                        break\n",
        "\n",
        "\n",
        "\n",
        "                applicant = clean_text(applicant)\n",
        "\n",
        "\n",
        "            if petitioner==None  and fake_table_condition==True:\n",
        "                indices = find_element_index(extracted_text,\"petitioner\")\n",
        "                if indices!=[] and len(extracted_text)!=1:\n",
        "                    petitioner = extracted_text[-1]\n",
        "                elif indices!=[] and len(extracted_text)==1:\n",
        "                    cropped_img_thresh,extracted_text_thresh,all_lines_thresh,all_bboxes_thresh = try_with_thresholding(cropped_img)\n",
        "                    indices = find_element_index(extracted_text_thresh,\"petitioner\")\n",
        "                    petitioner = extracted_text_thresh[-1]\n",
        "                    if display_info==True:\n",
        "                        print('From Thresh Image:')\n",
        "                        print(extracted_text_thresh)\n",
        "                        display_multi(cropped_img_thresh)\n",
        "\n",
        "                petitioner = clean_text(petitioner)\n",
        "\n",
        "            if beneficiary==None  and fake_table_condition==True:\n",
        "                indices = find_element_index(extracted_text,\"beneficiary\")\n",
        "                if indices!=[] and len(extracted_text)!=1:\n",
        "                    beneficiary = extracted_text[-1]\n",
        "                elif indices!=[] and len(extracted_text)==1:\n",
        "                    cropped_img_thresh,extracted_text_thresh,all_lines_thresh,all_bboxes_thresh = try_with_thresholding(cropped_img)\n",
        "                    indices = find_element_index(extracted_text_thresh,\"beneficiary\")\n",
        "                    beneficiary = extracted_text_thresh[-1]\n",
        "                    if display_info==True:\n",
        "                        print('From Thresh Image:')\n",
        "                        print(extracted_text_thresh)\n",
        "                        display_multi(cropped_img_thresh)\n",
        "\n",
        "                beneficiary = clean_text(beneficiary)\n",
        "\n",
        "\n",
        "            if case_type_tables==None  and fake_table_condition==True:\n",
        "                case_type_tables,case_type_line = find_case_type_from_tables(extracted_text,all_case_types)\n",
        "                if case_type_tables==None and find_element_index(extracted_text,'Case Type')!=[] or find_element_index(extracted_text,'CaseType')!=[]:\n",
        "                    cropped_img_thresh,extracted_text_thresh,all_lines_thresh,all_bboxes_thresh = try_with_thresholding(cropped_img)\n",
        "                    case_type_tables,case_type_line = find_case_type_from_tables(extracted_text_thresh,all_case_types)\n",
        "                    if display_info==True:\n",
        "                        print('From Thresh Image:')\n",
        "                        print(extracted_text_thresh)\n",
        "                        display_multi(cropped_img_thresh)\n",
        "\n",
        "            if notice_type==None  and fake_table_condition==True:\n",
        "                notice_type = find_notice_tags(notice_types_tables,extracted_text)\n",
        "                if notice_type==None and (find_element_index(extracted_text,\"Notice Type\")!=[] or find_element_index(extracted_text,\"NoticeType\")!=[]):\n",
        "                    '''if notice type is found but text under it is not found(happens sometimes in case of receipt) then try thresholding'''\n",
        "                    cropped_img_thresh,extracted_text_thresh,all_lines_thresh,all_bboxes_thresh = try_with_thresholding(cropped_img)\n",
        "                    notice_type = find_notice_tags(notice_types_tables,extracted_text_thresh)\n",
        "\n",
        "                    if display_info==True:\n",
        "                        print('From Thresh Image:')\n",
        "                        print(extracted_text_thresh)\n",
        "                        display_multi(cropped_img_thresh)\n",
        "\n",
        "\n",
        "            if page_num==None  and fake_table_condition==True:\n",
        "                indices = find_element_index_complete(extracted_text,\"page\")\n",
        "                if indices!=[]:\n",
        "                    '''\n",
        "                    sometimes page number page k agay bhe likha ho ga us ka kia karay ga bc\n",
        "                    '''\n",
        "                    page_num = extracted_text[indices+1]\n",
        "                    page_num = page_num.strip()[0]\n",
        "\n",
        "\n",
        "    all_tables_info = {}\n",
        "    all_tables_info['name_co'] = name_co\n",
        "    all_tables_info['co'] = co\n",
        "    # all_tables_info['case_type'] = case_type\n",
        "    all_tables_info['case_type_tables'] = case_type_tables\n",
        "    all_tables_info['notice_type'] = notice_type\n",
        "\n",
        "    all_tables_info['applicant'] = applicant\n",
        "    all_tables_info['petitioner'] = petitioner\n",
        "    all_tables_info['beneficiary'] = beneficiary\n",
        "\n",
        "    all_tables_info['page_num'] = page_num\n",
        "\n",
        "    return all_tables_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKKU9d6gNBAE"
      },
      "source": [
        "## Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfetuUkmGhr6",
        "outputId": "441bdc79-c7e7-4f85-d2b6-c5a175018355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting common_functions.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile common_functions.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "import pytesseract\n",
        "import re\n",
        "import math\n",
        "from pytesseract import Output\n",
        "from utils.load_and_preprocess_utils import *\n",
        "\n",
        "def remove_items(lst, value):\n",
        "    return [item for item in lst if item != value]\n",
        "\n",
        "\n",
        "def delete_characters(in_string,del_char_list):\n",
        "    assert (type(del_char_list)==list or del_char_list==None) and (type(in_string)==str)\n",
        "    # Filter multiple characters from string\n",
        "    filtered_chars = filter(lambda item: item not in del_char_list, in_string)\n",
        "    # Join remaining characters in the filtered list\n",
        "    out_string = ''.join(filtered_chars)\n",
        "\n",
        "    return out_string\n",
        "\n",
        "# ## same as delete_characters\n",
        "# def remove_multiple_characters(in_string,in_remove):\n",
        "#     return in_string.translate({ord(i): None for i in in_remove})\n",
        "\n",
        "\n",
        "def find_element_index(in_list,in_str,del_char_list=None,only_first=True):\n",
        "    assert type(del_char_list)==str or del_char_list==None\n",
        "    '''\n",
        "    sees if a string lies in a line or not\n",
        "    '''\n",
        "    ## returns only the index of an input string(partial matching)\n",
        "    ## for now [0]=Returning only the first occurance\n",
        "    ## keep in mind s.lower()\n",
        "    if del_char_list==None:\n",
        "        indices = [i for i, s in enumerate(in_list) if in_str.lower() in s.lower()]\n",
        "    else:\n",
        "        indices = [i for i, s in enumerate(in_list) if delete_characters(in_str.lower(),del_char_list.lower()) in s.lower()]\n",
        "\n",
        "    if indices!=[]:\n",
        "        if only_first==True:\n",
        "            indices = indices[0]\n",
        "\n",
        "    return indices\n",
        "\n",
        "def find_element_index_complete(in_list,in_str,only_first=True):\n",
        "    '''\n",
        "    used for document type as it matches the WHOLE line with a string\n",
        "    '''\n",
        "    ## returns only the index of an input string(partial matching)\n",
        "    ## for now [0]=Returning only the first occurance\n",
        "    ## keep in mind s.lower()\n",
        "    indices = [i for i, s in enumerate(in_list) if in_str.lower() == s.lower()]\n",
        "\n",
        "    if indices!=[]:\n",
        "        if only_first==True:\n",
        "            indices = indices[0]\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def find_key_by_val(in_dict,in_val):\n",
        "    value = [i for i in in_dict if in_dict[i].lower()==in_val.lower()]\n",
        "    return value[0]\n",
        "\n",
        "\n",
        "def delete_empty_lines(extracted_text):\n",
        "    extracted_text = extracted_text.split('\\n')\n",
        "    extracted_text = list(filter(lambda x: x not in ['',' ','\\x0c'], extracted_text))\n",
        "    return extracted_text\n",
        "\n",
        "def clean_text(in_text):\n",
        "    if in_text == None:\n",
        "        return in_text\n",
        "    '''\n",
        "    trying to clean in_text by first spliting the string by ' '(space)\n",
        "    then removing the entries which have length=1\n",
        "    '''\n",
        "    idx_to_del = []\n",
        "    sp_in_text = in_text.split(' ')\n",
        "    for i in range(0,len(sp_in_text)):\n",
        "        if len(sp_in_text[i])==1:\n",
        "            idx_to_del.append(i)\n",
        "\n",
        "    new_in_text = del_list_indexes(sp_in_text, idx_to_del)\n",
        "\n",
        "    clean_in_text = ''\n",
        "    for i in range(0,len(new_in_text)):\n",
        "        if new_in_text[i]!='':\n",
        "            if i<len(new_in_text)-1:\n",
        "                clean_in_text+=new_in_text[i]+' '\n",
        "            else:\n",
        "                clean_in_text+=new_in_text[i]\n",
        "\n",
        "    return clean_in_text\n",
        "\n",
        "def delete_list_indices(in_list,unwanted_indices):\n",
        "    '''\n",
        "    delete indices from a list\n",
        "    '''\n",
        "    for ele in sorted(unwanted_indices, reverse = True):\n",
        "        del in_list[ele]\n",
        "\n",
        "    return in_list\n",
        "\n",
        "def del_list_indexes(re, id_to_del):\n",
        "    somelist = [i for j, i in enumerate(re) if j not in set(id_to_del)]\n",
        "    return somelist\n",
        "\n",
        "def clean_the_data_dict(data_dict,del_list=['',' ']):\n",
        "    for del_char in del_list:\n",
        "        empty_indices = find_element_index_complete(data_dict['text'],del_char,only_first=False)\n",
        "        for k in data_dict:\n",
        "            data_dict[k] = delete_list_indices(data_dict[k],empty_indices)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def erode_and_find_contours(img,erode_iters=10):\n",
        "    ''' for further narrowing the cropped table cell '''\n",
        "    ''' Specially filtered for name_co wala contour'''\n",
        "    eroded_img = generate_contour_image(img,iterations=erode_iters)\n",
        "    e_img_h,e_img_w = eroded_img.shape[:2]\n",
        "    contours = find_contour_bboxes(eroded_img)\n",
        "    if len(contours)>1:\n",
        "        filtered_contours = filter_contour_indices(contours,e_img_h,e_img_w,y_limit=0.4,x_limit=0.4,custom_filters=filter_for_all_contours)\n",
        "        all_contour_imgs = filter_contours(img,contours,filtered_contours=filtered_contours)\n",
        "    elif len(contours)==1:\n",
        "        all_contour_imgs = filter_contours(img,contours,filtered_contours=None)\n",
        "    else:\n",
        "        all_contour_imgs = []\n",
        "\n",
        "    return all_contour_imgs,eroded_img\n",
        "\n",
        "def filter_for_all_contours(x,y,w,h,y_limit,x_limit,img_h,img_w): ## 50 is in case of resize==2\n",
        "    return (x<=(img_w*x_limit)) and (w>=h)  and (y<(img_h*y_limit)) and ((y+h)>40) and ((x+w)>50)\n",
        "\n",
        "\n",
        "def generate_contour_image(img,iterations=10,kernel=np.ones((5,5),np.uint8)):\n",
        "    ## erode since the image has black text and white bg\n",
        "    eroded_img = cv2.erode(img,kernel,iterations=iterations)\n",
        "    eroded_img = cv2.bitwise_not(eroded_img)\n",
        "    ret, eroded_img = cv2.threshold(eroded_img, 150, 255, cv2.THRESH_BINARY)\n",
        "    return eroded_img\n",
        "\n",
        "def find_contour_bboxes(eroded_img):\n",
        "    contours, hierarchy = cv2.findContours(eroded_img,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    return contours\n",
        "\n",
        "def crop_contour(img,contours,cont_id):\n",
        "    x,y,w,h = cv2.boundingRect(contours[cont_id])\n",
        "    cnt_img = img[y:y+h,x:x+w]\n",
        "    return cnt_img\n",
        "\n",
        "def filter_contours(img,contours,filtered_contours=None):\n",
        "    all_contours = []\n",
        "    for i in range(0,len(contours)):\n",
        "        if filtered_contours!=None:\n",
        "            if i in filtered_contours:\n",
        "                all_contours.append(crop_contour(img,contours,i))\n",
        "        else:\n",
        "            all_contours.append(crop_contour(img,contours,i))\n",
        "\n",
        "    return all_contours\n",
        "\n",
        "def filter_contour_indices(contours,img_h,img_w,y_limit,x_limit,custom_filters):\n",
        "    filtered_contours = []\n",
        "    for i in range(0,len(contours)):\n",
        "        x,y,w,h = cv2.boundingRect(contours[i])\n",
        "        if custom_filters(x,y,w,h,y_limit,x_limit,img_h,img_w)==True:\n",
        "            filtered_contours.append(i)\n",
        "\n",
        "    return filtered_contours\n",
        "\n",
        "def in_filters(x,y,w,h,y_limit,x_limit,img_h,img_w):\n",
        "    return (x<=(img_w*x_limit)) and (w>=h)  and ((y+h)<(img_h*y_limit))\n",
        "\n",
        "\n",
        "def calculate_distance(pt_1,pt_2):\n",
        "    return round(math.dist(pt_1,pt_2))\n",
        "\n",
        "\n",
        "def find_name_co_v2(all_lines_str,all_lines,all_bboxes,dist_limit=50):\n",
        "    co_list = None\n",
        "    name_co = None\n",
        "    for in_str in ['C/O','c/o','C/o','c/O','clo','c1o']:\n",
        "        indices = find_element_index(all_lines_str,in_str)\n",
        "        if indices!=[]:\n",
        "            ## removing everything except alphanueric and / because / comes in c/o\n",
        "            co_line = re.sub(r'[^\\w\\s\\/]', '', all_lines_str[indices])\n",
        "\n",
        "            if co_line.strip().lower()[:3] != in_str.lower():\n",
        "                ## meaning C/O: doesn't appear at the start of the line\n",
        "                return None,None\n",
        "            else:\n",
        "                bad_dist_indices = check_coordinates_consecutive(all_bboxes,indices,dist_limit=dist_limit)\n",
        "                co_list = delete_list_indices(all_lines[indices],bad_dist_indices)\n",
        "                co_list = ' '.join(co_list)\n",
        "\n",
        "        if co_list!=None:\n",
        "            ## C/O found\n",
        "            bad_dist_indices = check_coordinates_consecutive(all_bboxes,indices-1,dist_limit=dist_limit)\n",
        "            name_co = delete_list_indices(all_lines[indices-1],bad_dist_indices)\n",
        "            name_co = ' '.join(name_co)\n",
        "            break\n",
        "\n",
        "    return co_list,name_co\n",
        "\n",
        "def text_to_old_format(data_dict):\n",
        "    one_line = []\n",
        "    all_lines = []\n",
        "\n",
        "    bbox_line = []\n",
        "    all_bboxes = []\n",
        "\n",
        "    one_line_str = ''\n",
        "    all_lines_str = []\n",
        "    for i in range(0,len(data_dict['text'])):\n",
        "    # for i in tqdm(range(15,40)):\n",
        "        if data_dict['word_num'][i]==1:\n",
        "            if i!=0:\n",
        "                all_lines.append(one_line)\n",
        "                all_lines_str.append(one_line_str)\n",
        "                all_bboxes.append(bbox_line)\n",
        "\n",
        "            one_line = [data_dict['text'][i]]\n",
        "            one_line_str = data_dict['text'][i]\n",
        "            bbox_line = [(data_dict['left'][i],data_dict['top'][i],data_dict['width'][i],data_dict['height'][i])]\n",
        "        else:\n",
        "            one_line_str = one_line_str+' '+data_dict['text'][i]\n",
        "            one_line.append(data_dict['text'][i])\n",
        "            bbox_line.append((data_dict['left'][i],data_dict['top'][i],data_dict['width'][i],data_dict['height'][i]))\n",
        "\n",
        "    all_lines.append(one_line)\n",
        "    all_lines_str.append(one_line_str)\n",
        "    all_bboxes.append(bbox_line)\n",
        "\n",
        "    return all_lines_str,all_lines,all_bboxes\n",
        "\n",
        "\n",
        "def try_with_thresholding(cropped_img):\n",
        "    cropped_img_thresh = skimage.util.img_as_ubyte(cropped_img > skimage.filters.threshold_li(cropped_img))\n",
        "    data_dict_thresh = pytesseract.image_to_data(cropped_img_thresh, output_type=pytesseract.Output.DICT)\n",
        "    data_dict_thresh = clean_the_data_dict(data_dict_thresh,del_list=['',' '])\n",
        "    extracted_text_thresh,all_lines_thresh,all_bboxes_thresh = text_to_old_format(data_dict_thresh)\n",
        "\n",
        "    return cropped_img_thresh,extracted_text_thresh,all_lines_thresh,all_bboxes_thresh\n",
        "\n",
        "\n",
        "def check_coordinates_consecutive(all_bboxes,indices,dist_limit=50):\n",
        "    '''\n",
        "    measures the distance between the bboxes of detected words\n",
        "    if distance is > than a threshold then the two words don't belong to same group\n",
        "    specificaly made for name in front of RE: . E.g. RE: XYZ junk 412-213-\n",
        "    So sometimes the junk values are also detected as part of name\n",
        "    '''\n",
        "    bad_dist_indices = []\n",
        "    for bb in range(0,len(all_bboxes[indices])-1):\n",
        "        ## (left,top,width,height)\n",
        "        lft_1,top_1,wid_1,hei_1 = all_bboxes[indices][bb]\n",
        "        lft_2,top_2,wid_2,hei_2 = all_bboxes[indices][bb+1]\n",
        "        bb_dist = calculate_distance((lft_1+wid_1,top_1),(lft_2,top_2))\n",
        "        if bb_dist>dist_limit:\n",
        "            bad_dist_indices.append(bb+1)\n",
        "\n",
        "\n",
        "    return bad_dist_indices\n",
        "\n",
        "\n",
        "def find_case_type_v2(all_lines_str,all_lines,all_bboxes,all_case_types,in_str,dist_limit=30):\n",
        "    '''\n",
        "    finds RE: and then finds case types from case type list. If case type distance <thres with RE then case type is returned\n",
        "    '''\n",
        "    (indices_1,in_str_idx_1),(lft_1,top_1,wid_1,hei_1) = get_word_idx_line_pos(all_lines_str,all_lines,all_bboxes,in_str)\n",
        "\n",
        "    # found_tags = []\n",
        "    if indices_1!=[]:\n",
        "        for in_tag in all_case_types:\n",
        "            try:\n",
        "                (indices_2,in_str_idx_2),(lft_2,top_2,wid_2,hei_2) = get_word_idx_line_pos(all_lines_str,all_lines,all_bboxes,in_tag)\n",
        "                if indices_2!=[]:\n",
        "                    bb_dist = calculate_distance((lft_1,top_1+hei_1),(lft_2,top_2))\n",
        "                    if bb_dist<dist_limit:\n",
        "                        return in_tag, all_lines[indices_2][in_str_idx_2]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return None,None\n",
        "\n",
        "def get_word_idx_line_pos(all_lines_str,all_lines,all_bboxes,in_str):\n",
        "    '''\n",
        "    finds the line number and word position of a given string\n",
        "    '''\n",
        "    indices = find_element_index(all_lines_str,in_str)\n",
        "    if indices!=[]:\n",
        "        in_str_idx = find_element_index(all_lines[indices],in_str)\n",
        "        lft_1,top_1,wid_1,hei_1 = all_bboxes[indices][in_str_idx]\n",
        "        return (indices,in_str_idx),(lft_1,top_1,wid_1,hei_1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def find_a_file_num_name(all_lines_str):\n",
        "    '''In few docs there is A file number: xyz and then in above line there is name of the person '''\n",
        "    indices = find_element_index(all_lines_str,'A file number')\n",
        "    if indices!=[] and indices>0:\n",
        "        return all_lines_str[indices-1]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def find_attorney_copy(all_lines_str):\n",
        "    '''In few docs there is Attorney Copy written '''\n",
        "    indices = find_element_index(all_lines_str,'Attorney Copy')\n",
        "    if indices!=[]:\n",
        "        return True\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def filters_for_table_detection(x,y,w,h,y_limit,x_limit,img_h,img_w):\n",
        "    return (w>=(img_w*x_limit)) and (w>=h) and (w<(img_w*0.9))\n",
        "\n",
        "def filters_for_date_detection(x,y,w,h,y_limit,x_limit,img_h,img_w):\n",
        "    return (w<=(img_w*x_limit)) and (w>=2*h)\n",
        "\n",
        "def detect_tables_vs_nontables(img,display_info=False):\n",
        "    init_type = None\n",
        "\n",
        "    img = np.array(trim_image_borders(Image.fromarray(img)))\n",
        "\n",
        "    img_h,img_w = img.shape\n",
        "    # test_img = img[:int(img_h*0.06),int(img_w*0.5):]\n",
        "    test_img = img[:int(img_h*0.12),:]\n",
        "    test_img = skimage.util.img_as_ubyte(test_img > skimage.filters.threshold_otsu(test_img))\n",
        "    # extracted_text = pytesseract.image_to_string(test_img)\n",
        "    # extracted_text = delete_empty_lines(extracted_text)\n",
        "\n",
        "    data_dict = pytesseract.image_to_data(test_img, output_type=Output.DICT)\n",
        "    data_dict = clean_the_data_dict(data_dict,del_list=['',' '])\n",
        "    extracted_text,all_lines,all_bboxes = text_to_old_format(data_dict)\n",
        "\n",
        "\n",
        "\n",
        "    if display_info==True:\n",
        "        print(' ')\n",
        "        print('---------------------------------------')\n",
        "        print('Tables')\n",
        "    if display_info==True:\n",
        "        print(extracted_text)\n",
        "        display_multi(test_img)\n",
        "\n",
        "    for in_txt in ['Action','Does not grant']:\n",
        "        indices = find_element_index(extracted_text,in_txt)\n",
        "        if indices!=[]:\n",
        "            init_type = 'Tables'\n",
        "            break\n",
        "\n",
        "    if init_type==None:\n",
        "        indices = find_element_index(extracted_text,'IN THE DISTRICT COURT OF APPEAL')\n",
        "        if indices!=[]:\n",
        "            init_type = 'Unidentified'\n",
        "\n",
        "    if init_type==None:\n",
        "        #####################################################\n",
        "        ### Search for Normals in tables text too. For cases like oath ceremony and date that appease in a complete line\n",
        "\n",
        "        for in_txt in ['Oath','Oath Ceremony','Naturalization']:\n",
        "            indices = find_element_index(extracted_text,in_txt)\n",
        "            if indices!=[]:\n",
        "                init_type = 'Normal'\n",
        "                break\n",
        "\n",
        "        for in_txt in ['January','February','March','April','May','June','July','August','September','October','November','December']:\n",
        "            indices = find_element_index(extracted_text,in_txt)\n",
        "            if indices!=[]:\n",
        "                bad_dist_indices = check_coordinates_consecutive(all_bboxes,indices,dist_limit=50)\n",
        "\n",
        "            if indices!=[] and len(extracted_text[indices].split(' '))>5 and bad_dist_indices==[]:\n",
        "                init_type = 'Useless'\n",
        "                if display_info==True:\n",
        "                    print(extracted_text)\n",
        "                    print(extracted_text[indices])\n",
        "                    display_multi(test_img)\n",
        "                break\n",
        "\n",
        "            if indices!=[] and find_element_index(extracted_text[indices].split(' '),in_txt)!=0:\n",
        "                init_type = 'Useless'\n",
        "                if display_info==True:\n",
        "                    print(extracted_text)\n",
        "                    print(extracted_text[indices])\n",
        "                    display_multi(test_img)\n",
        "                break\n",
        "\n",
        "    #####################################################\n",
        "    if init_type==None:\n",
        "        eroded_img = cv2.erode(test_img,np.ones((1,5),np.uint8),iterations=5)\n",
        "        eroded_img = cv2.bitwise_not(eroded_img)\n",
        "        ret, eroded_img = cv2.threshold(eroded_img, 150, 255, cv2.THRESH_BINARY)\n",
        "        e_img_h,e_img_w = eroded_img.shape[:2]\n",
        "        contours = find_contour_bboxes(eroded_img)\n",
        "        filtered_contours = filter_contour_indices(contours,img_h,img_w,y_limit=None,x_limit=0.5,custom_filters=filters_for_table_detection)\n",
        "        all_contour_imgs = filter_contours(img,contours,filtered_contours=filtered_contours)\n",
        "        for cnt_img in all_contour_imgs:\n",
        "            cnt_img = add_image_border(cnt_img,10,255)\n",
        "            extracted_text = pytesseract.image_to_string(cnt_img)\n",
        "            extracted_text = delete_empty_lines(extracted_text)\n",
        "            if display_info==True:\n",
        "                display_multi(cnt_img)\n",
        "\n",
        "            for in_txt in ['Action','Does not grant']:\n",
        "                indices = find_element_index(extracted_text,in_txt)\n",
        "                if indices!=[]:\n",
        "                    init_type = 'Tables'\n",
        "                    break\n",
        "\n",
        "            if init_type!=None:\n",
        "                break\n",
        "\n",
        "    if display_info==True:\n",
        "        print(' ')\n",
        "        print('---------------------------------------')\n",
        "        print('Normal')\n",
        "\n",
        "\n",
        "    if init_type==None:\n",
        "\n",
        "        test_img = img[:int(img_h*0.15),:int(img_h*0.25)]\n",
        "        # test_img = skimage.util.img_as_ubyte(test_img > skimage.filters.threshold_otsu(test_img))\n",
        "        extracted_text = pytesseract.image_to_string(test_img)\n",
        "        extracted_text = delete_empty_lines(extracted_text)\n",
        "\n",
        "        if display_info==True:\n",
        "            print(extracted_text)\n",
        "            display_multi(test_img)\n",
        "\n",
        "        for in_txt in ['January','February','March','April','May','June','July','August','September','October','November','December',\n",
        "                        'Oath','Oath Ceremony','Naturalization']:\n",
        "            indices = find_element_index(extracted_text,in_txt)\n",
        "            if indices!=[] and (in_txt in ['Oath','Oath Ceremony','Naturalization']):\n",
        "                init_type = 'Normal'\n",
        "                break\n",
        "\n",
        "            elif indices!=[] and len(extracted_text[indices].split(' '))<5 and find_element_index(extracted_text[indices].split(' '),in_txt)==0:\n",
        "                init_type = 'Normal'\n",
        "                break\n",
        "\n",
        "    if init_type==None:\n",
        "        test_img = img[:int(img_h*0.15),:int(img_h*0.25)]\n",
        "        eroded_img = cv2.erode(test_img,np.ones((3,5),np.uint8),iterations=5)\n",
        "        eroded_img = cv2.bitwise_not(eroded_img)\n",
        "        ret, eroded_img = cv2.threshold(eroded_img, 150, 255, cv2.THRESH_BINARY)\n",
        "        e_img_h,e_img_w = eroded_img.shape[:2]\n",
        "        contours = find_contour_bboxes(eroded_img)\n",
        "        filtered_contours = filter_contour_indices(contours,img_h,img_w,y_limit=None,x_limit=0.4,custom_filters=filters_for_date_detection)\n",
        "        all_contour_imgs = filter_contours(img,contours,filtered_contours=filtered_contours)\n",
        "\n",
        "        for cnt_img in all_contour_imgs:\n",
        "            cnt_img = add_image_border(cnt_img,10,255)\n",
        "            extracted_text = pytesseract.image_to_string(cnt_img)\n",
        "            extracted_text = delete_empty_lines(extracted_text)\n",
        "            if display_info==True:\n",
        "                print(extracted_text)\n",
        "                display_multi(cnt_img)\n",
        "            for in_txt in ['January','February','March','April','May','June','July','August','September','October','November','December',\n",
        "                        'Oath','Oath Ceremony','Naturalization']:\n",
        "                indices = find_element_index(extracted_text,in_txt)\n",
        "                if indices!=[] and (in_txt in ['Oath','Oath Ceremony','Naturalization']):\n",
        "                    init_type = 'Normal'\n",
        "                    break\n",
        "\n",
        "                elif indices!=[] and len(extracted_text[indices].split(' '))<5 and find_element_index(extracted_text[indices].split(' '),in_txt)==0:\n",
        "                    init_type = 'Normal'\n",
        "                    break\n",
        "\n",
        "            if init_type!=None:\n",
        "                break\n",
        "\n",
        "\n",
        "    # if init_type==None:\n",
        "    #     init_type = 'None'\n",
        "\n",
        "    return init_type\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upLSWXDX9VI0"
      },
      "source": [
        "## For Non Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl67FMSx8lAj",
        "outputId": "32a9a2de-b804-4715-ac5c-716ca3f90907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting non_table_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile non_table_utils.py\n",
        "\n",
        "from utils.common_functions import *\n",
        "\n",
        "import pytesseract\n",
        "import re\n",
        "import cv2\n",
        "\n",
        "import math\n",
        "from pytesseract import Output\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import skimage\n",
        "\n",
        "\n",
        "def find_re_v2(all_lines_str,all_lines,all_bboxes,in_str='RE:',dist_limit=200,dist_check='consecutive'):\n",
        "    assert dist_check=='fixed' or dist_check=='consecutive'\n",
        "    indices = find_element_index(all_lines_str,in_str)\n",
        "\n",
        "    if indices!=[] and (all_lines_str[indices].strip().lower()[:3] != in_str.lower()):\n",
        "        ## meaning RE: doesn't appear at the start of the line\n",
        "        return None\n",
        "\n",
        "    if indices!=[] and dist_check=='fixed':\n",
        "        in_str_idx = find_element_index(all_lines[indices],in_str)\n",
        "\n",
        "    if indices!=[] and dist_check=='fixed':\n",
        "        bad_dist_indices = check_coordinates_fixed(all_bboxes,indices,in_str_idx,dist_limit=dist_limit)\n",
        "        ## remove the RE iteself too\n",
        "        # bad_dist_indices.append(in_str_idx)\n",
        "        re_list = delete_list_indices(all_lines[indices],bad_dist_indices)\n",
        "        re_list = ' '.join(re_list)\n",
        "\n",
        "    elif indices!=[] and dist_check=='consecutive':\n",
        "        bad_dist_indices = check_coordinates_consecutive(all_bboxes,indices,dist_limit=dist_limit)\n",
        "        re_list = delete_list_indices(all_lines[indices],bad_dist_indices)\n",
        "        re_list = ' '.join(re_list)\n",
        "    else:\n",
        "        re_list = None\n",
        "\n",
        "    return re_list\n",
        "\n",
        "\n",
        "def check_coordinates_fixed(all_bboxes,indices,in_str_idx,dist_limit=200):\n",
        "    '''\n",
        "    measures the distance between the bboxes of detected words\n",
        "    if distance is > than a threshold then the two words don't belong to same group\n",
        "    specificaly made for name in front of RE: . E.g. RE: XYZ junk 412-213-\n",
        "    So sometimes the junk values are also detected as part of name\n",
        "    '''\n",
        "    lft_1,top_1,wid_1,hei_1 = all_bboxes[indices][in_str_idx]\n",
        "    bad_dist_indices = []\n",
        "    for bb in range(0,len(all_bboxes[indices])):\n",
        "        if bb!=in_str_idx:\n",
        "            ## (left,top,width,height)\n",
        "            lft_2,top_2,wid_2,hei_2 = all_bboxes[indices][bb]\n",
        "            bb_dist = calculate_distance((lft_1+wid_1,top_1),(lft_2,top_2))\n",
        "            if bb_dist>dist_limit:\n",
        "                bad_dist_indices.append(bb)\n",
        "\n",
        "\n",
        "    return bad_dist_indices\n",
        "\n",
        "\n",
        "def detect_and_extract_document_type(image,config,all_tags,all_tags_partial,adjust_img=True):\n",
        "    im_h,im_w = image.shape\n",
        "\n",
        "    if adjust_img==True:\n",
        "        ## in_image = image[int(im_h*0.25):int(im_h*0.5) , int(im_w*0.0):int(im_w*0.8)] ## adjust for document type\n",
        "        in_image = image[int(im_h*0.0):int(im_h*0.5) , int(im_w*0.0):int(im_w*0.8)] ## adjust for document type\n",
        "    else:\n",
        "        in_image = image\n",
        "\n",
        "    data_dict = pytesseract.image_to_data(in_image, output_type=pytesseract.Output.DICT)\n",
        "    data_dict = clean_the_data_dict(data_dict,del_list=['',' '])\n",
        "    extracted_text,all_lines,all_bboxes = text_to_old_format(data_dict)\n",
        "\n",
        "    if extracted_text != '\\x0c':\n",
        "        # extracted_text = delete_empty_lines(extracted_text)\n",
        "        doc_type = match_document_tags(all_tags,all_tags_partial,extracted_text)\n",
        "    else:\n",
        "        doc_type = None\n",
        "\n",
        "\n",
        "    return doc_type,(in_image,extracted_text,all_lines,all_bboxes)\n",
        "\n",
        "def match_document_tags(all_tags,all_tags_partial,extracted_text):\n",
        "    '''\n",
        "    finds tag lines in extracted text\n",
        "    '''\n",
        "    for tag in list(all_tags.values()):\n",
        "        if tag not in all_tags_partial:\n",
        "            det_idx = find_element_index_complete(extracted_text,tag)\n",
        "        else:\n",
        "            det_idx = find_element_index(extracted_text,tag)\n",
        "\n",
        "        if det_idx!=[]:\n",
        "            return find_key_by_val(all_tags,tag)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_dear_name(extracted_text):\n",
        "    '''\n",
        "    if RE wasn't found then searches for dear NAME (can be seen in NIVCC)\n",
        "    '''\n",
        "    det_dear_name = None\n",
        "    det_idx = find_element_index(extracted_text,'Dear')\n",
        "    if det_idx!=[]:\n",
        "        det_dear_name = extracted_text[det_idx].strip('Dear').strip(' ').replace(':','')\n",
        "\n",
        "\n",
        "    return det_dear_name\n",
        "\n",
        "def img_to_all_data_dicts(img,do_filtering=True,erode_iters=10):\n",
        "    # in_img_thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "    # in_img_thresh = skimage.filters.threshold_mean(img)\n",
        "    # in_img_thresh = img > in_img_thresh\n",
        "\n",
        "    img_h,img_w = img.shape\n",
        "    all_data_dicts = []\n",
        "\n",
        "    eroded_img = generate_contour_image(img,iterations=erode_iters)\n",
        "    contours = find_contour_bboxes(eroded_img)\n",
        "    if do_filtering==True:\n",
        "        filtered_contours = filter_contour_indices(contours,img_h,img_w,y_limit=0.6,x_limit=1.0,custom_filters=in_filters)\n",
        "    else:\n",
        "        filtered_contours = None\n",
        "\n",
        "    all_contour_imgs = filter_contours(img,contours,filtered_contours=filtered_contours)\n",
        "\n",
        "    for cont_idx in range(0,len(all_contour_imgs)):\n",
        "        in_img_thresh = all_contour_imgs[cont_idx]\n",
        "        data_dict = pytesseract.image_to_data(in_img_thresh, output_type=Output.DICT)\n",
        "        data_dict = clean_the_data_dict(data_dict,del_list=['',' '])\n",
        "        all_lines_str,all_lines,all_bboxes = text_to_old_format(data_dict)\n",
        "        all_data_dicts.append((all_lines_str,all_lines,all_bboxes))\n",
        "\n",
        "    return all_data_dicts\n",
        "\n",
        "\n",
        "def data_dict_to_data(all_data_dicts,all_doc_tags,all_doc_tags_partial,all_case_types):\n",
        "    doc_type = None\n",
        "    re = None\n",
        "    case_type = None\n",
        "    dear_name = None\n",
        "    co = None\n",
        "    name_co = None\n",
        "    a_file_num_name = None\n",
        "    attorney_copy = None\n",
        "    case_type_attorney = None\n",
        "\n",
        "    all_doc_info = {}\n",
        "    for data_idx in range(0,len(all_data_dicts)):\n",
        "        (extracted_text,all_lines,all_bboxes) = all_data_dicts[data_idx]\n",
        "\n",
        "        if doc_type==None:\n",
        "            doc_type = match_document_tags(all_doc_tags,all_doc_tags_partial,extracted_text)\n",
        "\n",
        "        if re==None:\n",
        "            for re_end in [':',';']:\n",
        "                re = find_re_v2(extracted_text,all_lines,all_bboxes,in_str='RE'+re_end,dist_check='consecutive',dist_limit=50)## dist_limit depends on image size\n",
        "                if re!=None:\n",
        "                    re = re.replace('RE','').strip(re_end).strip()\n",
        "                    if case_type==None:\n",
        "                        case_type, case_type_org = find_case_type_v2(extracted_text,all_lines,all_bboxes,all_case_types,'RE'+re_end,dist_limit=30)\n",
        "                if re!=None:\n",
        "                    break\n",
        "\n",
        "        if dear_name==None:\n",
        "            dear_name = find_dear_name(extracted_text)\n",
        "\n",
        "        if co==None or name_co==None:\n",
        "            co,name_co = find_name_co_v2(extracted_text,all_lines,all_bboxes,dist_limit=30)\n",
        "\n",
        "        if a_file_num_name==None:\n",
        "            a_file_num_name = find_a_file_num_name(extracted_text)\n",
        "            if a_file_num_name!=None:\n",
        "                '''In few cases there is RE: before the name '''\n",
        "                a_file_num_name = a_file_num_name.strip('RE:')\n",
        "                a_file_num_name = a_file_num_name.strip('RE;')\n",
        "                a_file_num_name = a_file_num_name.strip()\n",
        "\n",
        "        if attorney_copy==None:\n",
        "            attorney_copy = find_attorney_copy(extracted_text)\n",
        "\n",
        "        if attorney_copy!=None and case_type_attorney==None:\n",
        "            for data_idx_atc in range(0,len(all_data_dicts)):\n",
        "                (extracted_text_atc,all_lines_atc,all_bboxes_atc) = all_data_dicts[data_idx_atc]\n",
        "\n",
        "                for ct_at in ['Case Type','CaseType']:\n",
        "                    indices_ct = find_element_index(extracted_text_atc,ct_at)\n",
        "                    if indices_ct!=[]:\n",
        "                        for c_t_2 in all_case_types:\n",
        "                            if c_t_2 in extracted_text_atc[indices_ct]:\n",
        "                                case_type_attorney = c_t_2\n",
        "                                break\n",
        "                    if case_type_attorney!=None:\n",
        "                        break\n",
        "                if case_type_attorney!=None:\n",
        "                        break\n",
        "\n",
        "    all_doc_info['doc_type'] = doc_type\n",
        "    all_doc_info['case_type'] = case_type\n",
        "    all_doc_info['re'] = re\n",
        "    all_doc_info['dear_name'] = dear_name\n",
        "    all_doc_info['co'] = co\n",
        "    all_doc_info['name_co'] = name_co\n",
        "    all_doc_info['a_file_num_name'] = a_file_num_name\n",
        "    all_doc_info['attorney_copy'] = attorney_copy\n",
        "    all_doc_info['case_type_attorney'] = case_type_attorney\n",
        "\n",
        "    return all_doc_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSuuNiiJxjJ"
      },
      "source": [
        "## Formatting Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGk9nm__J1Vo",
        "outputId": "cfaf7d75-835a-4ce2-9360-7335d0c4cce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting formatting_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile formatting_utils.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "from utils.common_functions import delete_characters\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfWriter, PdfReader\n",
        "\n",
        "def format_name(in_name):\n",
        "\n",
        "    if in_name==None:\n",
        "        return in_name\n",
        "\n",
        "    in_name = delete_characters(in_name,[',','.'])\n",
        "\n",
        "    in_name = in_name.split(' ')\n",
        "    if len(in_name)>1:\n",
        "        f_name = in_name[1]+', '+in_name[0]\n",
        "    else:\n",
        "        f_name = in_name[0]\n",
        "\n",
        "    return f_name\n",
        "\n",
        "def correct_final_doc_type(doc_type):\n",
        "    ''' if there is \"Receipt_2\" then make it \"Receipt\"   if its Decision_Notice then leave it as it is'''\n",
        "    if doc_type==None or doc_type=='Unidentified':\n",
        "        return doc_type\n",
        "\n",
        "    if '_' in doc_type:\n",
        "        if doc_type.split('_')[1] in ['0','1','2','3','4','5','6','7','8','9','10']:\n",
        "            doc_type = doc_type.split('_')[0]\n",
        "\n",
        "    return doc_type\n",
        "\n",
        "def correct_final_case_type(case_type):\n",
        "\n",
        "    if case_type!=None:\n",
        "        case_type = re.sub(r'[^\\w\\s\\/]', '',case_type)\n",
        "        if case_type=='400':\n",
        "            case_type = 'N-'+case_type\n",
        "        elif case_type!='NA' and case_type!='400':\n",
        "            case_type = 'I-'+case_type\n",
        "        else:\n",
        "            case_type = 'NA'\n",
        "\n",
        "    return case_type\n",
        "\n",
        "def decide_page(info_dict,info_dict_old,notice_types_tables,all_doc_tags):\n",
        "\n",
        "    ## tables\n",
        "    if (info_dict['doc_type']in['Interview','Interview_2']) and (info_dict_old['doc_type']in['Interview','Interview_2']) \\\n",
        "        and (info_dict['case_type']in['485','-485']) and (info_dict_old['case_type']in['485','-485']) \\\n",
        "        and (info_dict['name']==None) and (info_dict_old['name']!=None):\n",
        "        return True\n",
        "\n",
        "    elif (info_dict_old['doc_type']in notice_types_tables.keys()) and (info_dict['doc_type']==None) and (info_dict['case_type']==None) and (info_dict['name']==None):\n",
        "        return True\n",
        "\n",
        "    elif (info_dict['doc_type']==None) and (info_dict['case_type']==None) and (info_dict['name']==None) and \\\n",
        "        (info_dict_old['doc_type']==None) and (info_dict_old['case_type']==None) and (info_dict_old['name']==None):\n",
        "        return True\n",
        "\n",
        "    ## non tables\n",
        "    elif (info_dict['doc_type']==None) and (info_dict['case_type']==None) and (info_dict['name']==None) and \\\n",
        "        (info_dict_old['doc_type'] in all_doc_tags.keys()) and (info_dict_old['case_type']!=None) and (info_dict_old['name']!=None):\n",
        "        return True\n",
        "\n",
        "    elif (info_dict['doc_type']==None) and (info_dict['case_type']==None) and (info_dict['name']==None):\n",
        "        return True\n",
        "\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def final_naming_tables(all_tables_info):\n",
        "    name = None\n",
        "    case_type = None\n",
        "    doc_type = None\n",
        "\n",
        "    ## missing 'USCIS'\n",
        "    if all_tables_info['notice_type']in['Reopen','Transfer','Approval','Receipt','Receipt_2',\n",
        "                                        'Rejection','ASC','Interview','Interview_2','Cancellation','Applicants','Biometric']:\n",
        "        name = all_tables_info['name_co']\n",
        "        case_type = all_tables_info['case_type_tables']\n",
        "        doc_type = all_tables_info['notice_type']\n",
        "\n",
        "    elif all_tables_info['notice_type']=='USCIS':\n",
        "        name = all_tables_info['applicant']\n",
        "        case_type = all_tables_info['case_type_tables']\n",
        "        doc_type = all_tables_info['notice_type']\n",
        "\n",
        "\n",
        "    if (all_tables_info['name_co']!=None) and \\\n",
        "     ((all_tables_info['name_co'].lower() in 'SOLOMON IMMIGRATION LAW LLC'.lower())  or 'SOLOMON IMMIGRATION'.lower()in  all_tables_info['name_co'].lower()):\n",
        "        for nm in ['applicant','petitioner']:#'beneficiary'\n",
        "            if all_tables_info[nm]!=None:\n",
        "                name = all_tables_info[nm]\n",
        "                break\n",
        "\n",
        "\n",
        "    info_dict = {}\n",
        "    info_dict['name'] = name\n",
        "    info_dict['case_type'] = case_type\n",
        "    info_dict['doc_type'] = doc_type\n",
        "    # info_dict['page'] = 1\n",
        "    return info_dict\n",
        "\n",
        "\n",
        "def final_naming_nontables(all_doc_info):\n",
        "    name = None\n",
        "    case_type = None\n",
        "    doc_type = None\n",
        "\n",
        "    if all_doc_info['init_type']=='Unidentified':\n",
        "        name = 'Unidentified'\n",
        "        case_type = 'NA'\n",
        "        doc_type = 'NA'\n",
        "\n",
        "    elif (all_doc_info['doc_type']=='RFE') and (all_doc_info['case_type']in['-485','485']):\n",
        "        name = all_doc_info['re']\n",
        "        case_type = all_doc_info['case_type']\n",
        "        doc_type = all_doc_info['doc_type']\n",
        "\n",
        "    elif (all_doc_info['doc_type']=='RFE') and (all_doc_info['case_type']in['-129F','-130','129F','130']):\n",
        "        name = all_doc_info['name_co']\n",
        "        case_type = all_doc_info['case_type']\n",
        "        doc_type = all_doc_info['doc_type']\n",
        "\n",
        "    elif (all_doc_info['doc_type']=='Oath_Ceremony'):\n",
        "        name = all_doc_info['name_co']\n",
        "        case_type = 'N-400'#all_doc_info['case_type']\n",
        "        doc_type = all_doc_info['doc_type']\n",
        "\n",
        "    elif all_doc_info['doc_type'] in ['Deficiency_Notice','Courtesy_letter','Withdrawal','Decision','Decision_Notice','Withdrawal_Acknowledgment']:\n",
        "        if all_doc_info['name_co']!=None:\n",
        "            name = all_doc_info['name_co']\n",
        "        else:\n",
        "            name = all_doc_info['re']\n",
        "\n",
        "        case_type = all_doc_info['case_type']\n",
        "        doc_type = all_doc_info['doc_type']\n",
        "\n",
        "    elif (all_doc_info['doc_type']=='NIVCC'):\n",
        "        name = all_doc_info['dear_name']\n",
        "        case_type = 'NA'#all_doc_info['case_type']\n",
        "        doc_type = all_doc_info['doc_type']\n",
        "\n",
        "    elif all_doc_info['doc_type']==None and all_doc_info['a_file_num_name']!=None:\n",
        "        name = all_doc_info['a_file_num_name']\n",
        "        case_type = 'NA'\n",
        "        doc_type = 'NA'\n",
        "\n",
        "    elif all_doc_info['doc_type']==None and all_doc_info['dear_name']!=None and all_doc_info['case_type_attorney']==None \\\n",
        "            and ['Sir'.lower() not in all_doc_info['dear_name'].lower()] and ['Madam'.lower() not in all_doc_info['dear_name'].lower()]:\n",
        "        name = all_doc_info['dear_name']\n",
        "        case_type = 'NA'\n",
        "        doc_type = 'NA'\n",
        "\n",
        "    elif all_doc_info['doc_type']==None and all_doc_info['case_type_attorney']!=None:\n",
        "        name = all_doc_info['dear_name']\n",
        "        case_type = all_doc_info['case_type_attorney']\n",
        "        doc_type = 'NA'\n",
        "\n",
        "\n",
        "    info_dict = {}\n",
        "    info_dict['name'] = name\n",
        "    info_dict['case_type'] = case_type\n",
        "    info_dict['doc_type'] = doc_type\n",
        "    # info_dict['page'] = 1\n",
        "    return info_dict\n",
        "\n",
        "\n",
        "def make_pagewise_list(all_final_names,all_final_appends):\n",
        "    all_files_pagewise = []\n",
        "    for i in range(0,len(all_final_appends)):\n",
        "        one_file = []\n",
        "        if all_final_appends[i]==False:\n",
        "            curr_name = all_final_names[i]\n",
        "            one_file.append((curr_name,i))\n",
        "            for j in range(i+1,len(all_final_appends)):\n",
        "                if all_final_appends[j]==True:\n",
        "                    one_file.append(j)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            all_files_pagewise.append(one_file)\n",
        "\n",
        "    return all_files_pagewise\n",
        "\n",
        "def break_pdf_to_files(path_in_pdf,path_write_pdf,all_files_pagewise,compress_pdf=False):\n",
        "    inputpdf = PdfReader(open(path_in_pdf, \"rb\"))\n",
        "    for i in range(0,len(all_files_pagewise)):\n",
        "        one_pdf = all_files_pagewise[i]\n",
        "        pdf_writer = PdfWriter()\n",
        "        pdf_name,_ = os.path.splitext(one_pdf[0][0].split('/')[-1])\n",
        "        pdf_name = os.path.join(path_write_pdf,pdf_name+'.pdf')\n",
        "\n",
        "        one_page = inputpdf.pages[one_pdf[0][1]]\n",
        "\n",
        "        if compress_pdf==True:\n",
        "            one_page.compress_content_streams()\n",
        "\n",
        "        pdf_writer.add_page(one_page)\n",
        "\n",
        "        for j in range(1,len(one_pdf)):\n",
        "            one_page = inputpdf.pages[one_pdf[j]]\n",
        "            if compress_pdf==True:\n",
        "                one_page.compress_content_streams()\n",
        "\n",
        "            pdf_writer.add_page(one_page)\n",
        "\n",
        "        with open(pdf_name, \"wb\") as outputStream:\n",
        "            pdf_writer.write(outputStream)\n",
        "\n",
        "def break_pdf_to_images(all_pages,path_write_images,all_files_pagewise,jpeg_quality=95):\n",
        "\n",
        "    for i in range(0,len(all_files_pagewise)):\n",
        "        one_pdf = all_files_pagewise[i]\n",
        "\n",
        "        pdf_name,_ = os.path.splitext(one_pdf[0][0].split('/')[-1])\n",
        "        pdf_name = os.path.join(path_write_images,pdf_name+'.jpg')\n",
        "\n",
        "        one_page = np.array(all_pages[one_pdf[0][1]])\n",
        "\n",
        "        final_img = np.array(one_page)\n",
        "\n",
        "        for j in range(1,len(one_pdf)):\n",
        "            one_page = all_pages[one_pdf[j]]\n",
        "            final_img = np.concatenate((final_img,one_page),axis=1)\n",
        "\n",
        "\n",
        "        cv2.imwrite(pdf_name,final_img,[cv2.IMWRITE_JPEG_QUALITY, jpeg_quality])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl2FDuGk9pQE"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/pdf_parsing/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnnczgA43T8H",
        "outputId": "3be87a84-f295-41a1-e1c3-737344fca378"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pdf_parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N_wck1Hm96Fe"
      },
      "outputs": [],
      "source": [
        "from pdf2image import convert_from_path\n",
        "# from google.colab import files\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import cv2\n",
        "\n",
        "import math\n",
        "# from pytesseract import Output\n",
        "\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "# for de skew\n",
        "# from deskew import determine_skew  ## gray image\n",
        "import numpy as np\n",
        "import skimage\n",
        "# from skimage.transform import rotate\n",
        "\n",
        "from PyPDF2 import PdfWriter, PdfReader\n",
        "\n",
        "\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# def cv2_imshow_rgb(img,resize=None):\n",
        "#     if resize!=None:\n",
        "#         img=cv2.resize(img,resize)\n",
        "\n",
        "\n",
        "#     cv2_imshow(cv2.cvtColor(np.uint8(img),cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def cv2_imshow_rgb(img,resize=None, figsize=(15,15)):\n",
        "    if resize!=None:\n",
        "        img=cv2.resize(img,resize)\n",
        "\n",
        "        #     cv2_imshow(cv2.cvtColor(np.uint8(img),cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(np.uint8(img))\n",
        "\n",
        "\n",
        "def display_multi(*images,resize=None, figsize=(15,15),bgr=False,axis=1):\n",
        "    if resize!=None:\n",
        "        res = np.array(cv2.resize(images[0],resize))\n",
        "    else:\n",
        "        res = np.array(images[0])\n",
        "\n",
        "    for i in range(1,len(images)):\n",
        "\n",
        "        if resize!=None:\n",
        "            res_img = np.array(cv2.resize(images[i],resize))\n",
        "        else:\n",
        "            res_img = np.array(images[i])\n",
        "\n",
        "        res = np.concatenate((res, res_img), axis=axis)\n",
        "\n",
        "    if bgr==True:\n",
        "        res = cv2.cvtColor(res,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return cv2_imshow_rgb(res,resize=None, figsize=figsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qphUyJiiKSab"
      },
      "outputs": [],
      "source": [
        "from utils.load_and_preprocess_utils import preprocess_image\n",
        "from utils.table_utils import extract_all_tables,cropped_tables_to_data\n",
        "from utils.non_table_utils import img_to_all_data_dicts,data_dict_to_data\n",
        "from utils.non_table_utils import erode_and_find_contours\n",
        "from utils.formatting_utils import *\n",
        "from utils.notice_and_case_types import *\n",
        "from utils.common_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zAA340odqifZ"
      },
      "outputs": [],
      "source": [
        "def process_and_save(all_pages,junk_folder,display_info=False,st_i=None,end_i=None,save_jpegs=False):\n",
        "    ## Settings\n",
        "\n",
        "    ## pre processing options\n",
        "    op_dpi = (300,300)\n",
        "    op_deskew = True\n",
        "    op_img_thresh = False   ## True makes I to 1 so keep it false\n",
        "    op_borders = (False,10,(0,0,0))\n",
        "\n",
        "    ## thresholding and morphology\n",
        "    # ['try_all_threshold','threshold_otsu','threshold_yen','threshold_isodata','threshold_li','threshold_local','threshold_minimum','threshold_mean', 'threshold_niblack','threshold_sauvola','threshold_triangle','apply_hysteresis_threshold','threshold_multiotsu']\n",
        "    thresh_filter = None#skimage.filters.threshold_otsu\n",
        "    op_morph = None #apply_morph(img,morph_operation=cv2.MORPH_CLOSE,kernel=(3,3),iterations=1)\n",
        "\n",
        "\n",
        "    all_final_names = []\n",
        "    all_final_appends = []\n",
        "    ## Init dictionaries\n",
        "    all_tables_info = {}\n",
        "    all_doc_info = {}\n",
        "\n",
        "    info_dict_old = {}\n",
        "    info_dict_old['name'] = None\n",
        "    info_dict_old['case_type'] = None\n",
        "    info_dict_old['doc_type'] = None\n",
        "\n",
        "    info_dict = {}\n",
        "    info_dict['name'] = None\n",
        "    info_dict['case_type'] = None\n",
        "    info_dict['doc_type'] = None\n",
        "\n",
        "    corrupts = []\n",
        "    if st_i==None:\n",
        "        st_i = 0\n",
        "    if end_i==None:\n",
        "        end_i = len(all_pages)\n",
        "\n",
        "    for i in tqdm(range(st_i,end_i)):\n",
        "        # try:\n",
        "        pg_num = i+1\n",
        "\n",
        "        ## Init dictionaries\n",
        "        all_tables_info = {}\n",
        "        all_doc_info = {}\n",
        "        info_dict = {}\n",
        "\n",
        "        sp_name = 'page_'+str(pg_num)\n",
        "        img = cv2.cvtColor(np.array(all_pages[i]),cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        try:\n",
        "            img = check_orientation_tesseract(np.array(img))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        img = preprocess_image(img,deskew=op_deskew,img_thresh=op_img_thresh,\\\n",
        "                                add_border=op_borders[0],border_width=op_borders[1],border_color=op_borders[2],img_type='gray')\n",
        "\n",
        "        img_h,img_w = img.shape\n",
        "\n",
        "        init_type = detect_tables_vs_nontables(img,display_info=False)\n",
        "        if display_info==True:\n",
        "            print('')\n",
        "            print('Table/Not: ',init_type)\n",
        "\n",
        "        ## ------------------------------------------\n",
        "        ### Detection using tables\n",
        "        ## ------------------------------------------\n",
        "        ## resize image to make sure the capital letters are 30pixels in height\n",
        "        if init_type=='Tables':\n",
        "            table_resize = 2\n",
        "            fake_table = True\n",
        "            all_tables,table_coordinates = extract_all_tables(img.copy(),sp_name,path_cropped=\"page_tables/\",filter_tables=True,\n",
        "                                                            save_crops=False,save_overlay=False,fake_table=fake_table,fake_line=False,\n",
        "                                                            resize=table_resize)\n",
        "\n",
        "\n",
        "            narrow_cell = True\n",
        "            all_tables_info = cropped_tables_to_data(all_tables,table_coordinates,fake_table,notice_types_tables,all_case_types_tables,thresh_filter=thresh_filter,\n",
        "                                        op_borders=10,display_info=display_info,op_morph=op_morph,narrow_cell=narrow_cell)\n",
        "\n",
        "            all_tables_info['init_type'] = init_type\n",
        "            # if all_tables_info['notice_type'] in notice_types_tables.keys():\n",
        "            info_dict = final_naming_tables(all_tables_info)\n",
        "\n",
        "        else:\n",
        "            ## ------------------------------------------\n",
        "            ### Detection without tables\n",
        "            ## ------------------------------------------\n",
        "            all_data_dicts = img_to_all_data_dicts(img)\n",
        "            all_doc_info = data_dict_to_data(all_data_dicts,all_doc_tags,all_doc_tags_partial,all_case_types_tables)\n",
        "            all_doc_info['init_type'] = init_type\n",
        "            info_dict = final_naming_nontables(all_doc_info)\n",
        "\n",
        "            if display_info==True:\n",
        "                print('----------------------')\n",
        "                print('All data dicts')\n",
        "                print('----------------------')\n",
        "                for d_d in all_data_dicts:\n",
        "                    print(d_d[0])\n",
        "\n",
        "        append_page = decide_page(info_dict,info_dict_old,notice_types_tables,all_doc_tags)\n",
        "        ## update old dict\n",
        "        info_dict_old = info_dict\n",
        "\n",
        "        ## Extract important info\n",
        "        final_name = info_dict['name']\n",
        "        case_type = info_dict['case_type']\n",
        "        doc_type = info_dict['doc_type']\n",
        "\n",
        "        ## Correct final formats\n",
        "        final_name = format_name(final_name)\n",
        "        case_type = correct_final_case_type(case_type)\n",
        "        doc_type = correct_final_doc_type(doc_type)\n",
        "\n",
        "        ##############################################\n",
        "        ## saving images\n",
        "\n",
        "        save_name_full = None\n",
        "\n",
        "        save_name_full = str(pg_num)+'_'+str(final_name)+'_'+str(case_type)+'_'+str(doc_type)+\".jpg\"\n",
        "        save_name_full = os.path.join(junk_folder,save_name_full)\n",
        "        if save_jpegs==True:\n",
        "            cv2.imwrite(save_name_full,img,[cv2.IMWRITE_JPEG_QUALITY, 10])\n",
        "\n",
        "        all_final_names.append(save_name_full)\n",
        "        all_final_appends.append(append_page)\n",
        "\n",
        "        if display_info==True:\n",
        "            print(' ')\n",
        "            print(save_name_full)\n",
        "            print('Page:',append_page)\n",
        "            # print(all_tables_info)\n",
        "            print('-----------Tables-------------------')\n",
        "            for k in all_tables_info:\n",
        "                print(k,'==',all_tables_info[k])\n",
        "\n",
        "            print('-----------No Tables-------------------')\n",
        "            for k in all_doc_info:\n",
        "                print(k,'==',all_doc_info[k])\n",
        "\n",
        "            print('-----------Info Dict-------------------')\n",
        "            for k in info_dict:\n",
        "                print(k,'==',info_dict[k])\n",
        "\n",
        "            display_multi(img)\n",
        "\n",
        "        # except:\n",
        "        #     print('')\n",
        "        #     print('Corrupt:',pdf_name,pg_num)\n",
        "        #     corrupts.append((pdf_name,pg_num))\n",
        "\n",
        "    return all_final_names,all_final_appends,corrupts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aflHPkdxtmt",
        "outputId": "64ad7aff-302b-4174-b794-49219f92d96a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "glob('/content/*.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z-tCfcbIaZdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d03b0d7-e839-415e-d005-f0e2eb50f59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents: 40\n",
            "['/content/documents/AI Receipts Project/D-1.pdf', '/content/documents/AI Receipts Project/H-2.pdf', '/content/documents/AI Receipts Project/B-2.pdf', '/content/documents/AI Receipts Project/K-1.pdf', '/content/documents/AI Receipts Project/L-1.pdf', '/content/documents/AI Receipts Project/O-1.pdf', '/content/documents/AI Receipts Project/P-2.pdf', '/content/documents/AI Receipts Project/H-1.pdf', '/content/documents/AI Receipts Project/B-1.pdf', '/content/documents/AI Receipts Project/U-1.pdf', '/content/documents/AI Receipts Project/V-2.pdf', '/content/documents/AI Receipts Project/I-1.pdf', '/content/documents/AI Receipts Project/A-2.pdf', '/content/documents/AI Receipts Project/M-2.pdf', '/content/documents/AI Receipts Project/G-2.pdf', '/content/documents/AI Receipts Project/E-1.pdf', '/content/documents/AI Receipts Project/T-1.pdf', '/content/documents/AI Receipts Project/K-2.pdf', '/content/documents/AI Receipts Project/M-1.pdf', '/content/documents/AI Receipts Project/S-2.pdf', '/content/documents/AI Receipts Project/S-1.pdf', '/content/documents/AI Receipts Project/N-1.pdf', '/content/documents/AI Receipts Project/G-1.pdf', '/content/documents/AI Receipts Project/L-2.pdf', '/content/documents/AI Receipts Project/F-2.pdf', '/content/documents/AI Receipts Project/A-1.pdf', '/content/documents/AI Receipts Project/C-2.pdf', '/content/documents/AI Receipts Project/Y-1.pdf', '/content/documents/AI Receipts Project/J-2.pdf', '/content/documents/AI Receipts Project/F-1.pdf', '/content/documents/AI Receipts Project/P-1.pdf', '/content/documents/AI Receipts Project/J-1.pdf', '/content/documents/AI Receipts Project/D-2.pdf', '/content/documents/AI Receipts Project/W-1.pdf', '/content/documents/AI Receipts Project/R-1.pdf', '/content/documents/AI Receipts Project/V-1.pdf', '/content/documents/AI Receipts Project/Q-1.pdf', '/content/documents/AI Receipts Project/E-2.pdf', '/content/documents/AI Receipts Project/C-1.pdf', '/content/documents/AI Receipts Project/Z-1.pdf']\n",
            "/content/documents/AI Receipts Project/Z-1.pdf Z-1\n",
            "Size(Mb): 19\n",
            "Total Pages: 13\n"
          ]
        }
      ],
      "source": [
        "# all_documents = glob(\"/content/documents/forms/*\")\n",
        "all_documents = glob(\"/content/documents/AI Receipts Project/*\")\n",
        "\n",
        "print('Total Documents:',len(all_documents))\n",
        "print(all_documents)\n",
        "\n",
        "pdf_path = '/content/documents/AI Receipts Project/Z-1.pdf' #all_documents[1]\n",
        "pdf_name,_ = os.path.splitext(pdf_path.split('/')[-1])\n",
        "print(pdf_path,pdf_name)\n",
        "print('Size(Mb):',round(os.path.getsize(pdf_path)/1024/1024))\n",
        "\n",
        "## convert pdf to images\n",
        "all_pages = convert_from_path(pdf_path)\n",
        "print('Total Pages:',len(all_pages))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-d7AsZ0sYJ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd7d440-ae24-4a21-e745-a50efa0da9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [01:04<00:00, 12.91s/it]\n"
          ]
        }
      ],
      "source": [
        "junk_folder = os.path.join('/content/',pdf_name+'_junk/')\n",
        "!rm -r {junk_folder}\n",
        "\n",
        "if not os.path.isdir(junk_folder):\n",
        "    os.mkdir(junk_folder)\n",
        "\n",
        "# F2-18\n",
        "# S1-19\n",
        "\n",
        "st_i = 0\n",
        "num_forms = 5\n",
        "end_i = st_i+num_forms\n",
        "# end_i = len(all_pages)#st_i+num_forms #min(st_i+num_forms,len(selected_pages))\n",
        "\n",
        "# all_pages = [Image.open('/content/K1-3.png')]\n",
        "selected_pages = all_pages\n",
        "\n",
        "display_info=False\n",
        "save_jpegs = True\n",
        "all_final_names,all_final_appends,corrupts = process_and_save(all_pages,junk_folder,display_info=display_info,st_i=st_i,end_i=end_i,save_jpegs=save_jpegs)\n",
        "# display_multi(all_pages[st_i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RwM2SIQluyBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b140af5a-5718-4b49-dc22-f952f21f1268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/Z-1_images/ (stored 0%)\n",
            "updating: content/Z-1_images/5_ALBERTO, BRIANT_I-485_ASC.jpg (deflated 26%)\n",
            "updating: content/Z-1_images/4_A, BRIANT_I-485_Receipt.jpg (deflated 41%)\n",
            "updating: content/Z-1_images/3_A, BRIANT_I-131_Receipt.jpg (deflated 42%)\n",
            "updating: content/Z-1_images/2_A, BRIANT_I-765_Receipt.jpg (deflated 23%)\n",
            "updating: content/Z-1_images/1_CRAIG, ZELL_I-130_Approval.jpg (deflated 24%)\n"
          ]
        }
      ],
      "source": [
        "path_write_pdf = os.path.join('/content/',pdf_name+'_pdfs/')\n",
        "path_write_images = os.path.join('/content/',pdf_name+'_images/')\n",
        "\n",
        "!rm -r {path_write_pdf}\n",
        "!rm -r {path_write_images}\n",
        "\n",
        "if not os.path.isdir(path_write_pdf):\n",
        "    os.mkdir(path_write_pdf)\n",
        "\n",
        "\n",
        "if not os.path.isdir(path_write_images):\n",
        "    os.mkdir(path_write_images)\n",
        "\n",
        "all_files_pagewise = make_pagewise_list(all_final_names,all_final_appends)\n",
        "\n",
        "break_pdf_to_files(pdf_path,path_write_pdf,all_files_pagewise,compress_pdf=True)\n",
        "\n",
        "selected_pages = selected_pages[st_i:end_i]\n",
        "break_pdf_to_images(selected_pages,path_write_images,all_files_pagewise,jpeg_quality=10)\n",
        "\n",
        "zip_name = os.path.join('/content/',pdf_name+'.zip')\n",
        "!zip -r {zip_name} {path_write_images}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwnJjwUuwKhn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx7AVhl93RV4"
      },
      "outputs": [],
      "source": [
        "# all_documents = glob(\"/content/documents/forms/*\")\n",
        "all_documents = glob(\"/content/documents/AI Receipts Project/*\")\n",
        "\n",
        "print('Total Documents:',len(all_documents))\n",
        "print(all_documents)\n",
        "\n",
        "pdf_path = '/content/documents/AI Receipts Project/A-2.pdf' #all_documents[1]\n",
        "pdf_name,_ = os.path.splitext(pdf_path.split('/')[-1])\n",
        "print(pdf_path,pdf_name)\n",
        "print('Size(Mb):',round(os.path.getsize(pdf_path)/1024/1024))\n",
        "\n",
        "## convert pdf to images\n",
        "all_pages = convert_from_path(pdf_path)\n",
        "print('Total Pages:',len(all_pages))\n",
        "\n",
        "\n",
        "# # pdf_path = '/content/documents/131_raw/'\n",
        "# all_pages = glob(os.path.join(pdf_path,'*'))\n",
        "# pdf_name,_ = os.path.splitext(pdf_path.split('/')[-1])\n",
        "print(pdf_path,pdf_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXnalLOa13GA"
      },
      "outputs": [],
      "source": [
        "!rm -r '/content/tables/'\n",
        "path_save_types = '/content/tables/'\n",
        "if not os.path.isdir(path_save_types):\n",
        "    os.mkdir(path_save_types)\n",
        "\n",
        "\n",
        "## pre processing options\n",
        "op_dpi = (300,300)\n",
        "op_deskew = True\n",
        "op_img_thresh = False   ## True makes I to 1 so keep it false\n",
        "op_borders = (False,10,(0,0,0))\n",
        "\n",
        "## thresholding and morphology\n",
        "# ['try_all_threshold','threshold_otsu','threshold_yen','threshold_isodata','threshold_li','threshold_local','threshold_minimum','threshold_mean', 'threshold_niblack','threshold_sauvola','threshold_triangle','apply_hysteresis_threshold','threshold_multiotsu']\n",
        "thresh_filter = None#skimage.filters.threshold_otsu\n",
        "op_morph = None #apply_morph(img,morph_operation=cv2.MORPH_CLOSE,kernel=(3,3),iterations=1)\n",
        "\n",
        "selected_pages = all_pages.copy()\n",
        "st_i = 9\n",
        "num_forms = 1\n",
        "end_i = st_i+num_forms\n",
        "# end_i = len(selected_pages)#st_i+num_forms #min(st_i+num_forms,len(selected_pages))\n",
        "\n",
        "### selected_pages = selected_pages[st_i:end_i]\n",
        "print('start/end',st_i,end_i)\n",
        "display_info=True\n",
        "\n",
        "for j in range(st_i,end_i):\n",
        "    # img = cv2.imread(all_pages[j],0)\n",
        "    img = cv2.cvtColor(np.array(all_pages[j-1]),cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    try:\n",
        "        img = check_orientation_tesseract(np.array(img))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    img = preprocess_image(img,deskew=op_deskew,img_thresh=op_img_thresh,\\\n",
        "                            add_border=op_borders[0],border_width=op_borders[1],border_color=op_borders[2],img_type='gray')\n",
        "\n",
        "    img_h,img_w = img.shape\n",
        "    init_type = detect_tables_vs_nontables(img,display_info=display_info)\n",
        "\n",
        "    save_img_name = os.path.join(path_write_images,pdf_name,str(j+1)+'_page_'+init_type+'.jpg')\n",
        "    cv2.imwrite(save_img_name,np.array(all_pages[j]),[cv2.IMWRITE_JPEG_QUALITY, 10])\n",
        "\n",
        "print(init_type)\n",
        "display_multi(img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_H8ML072-Iyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbGXIg_B6QQE"
      },
      "outputs": [],
      "source": [
        "## pre processing options\n",
        "op_dpi = (300,300)\n",
        "op_deskew = True\n",
        "op_img_thresh = False   ## True makes I to 1 so keep it false\n",
        "op_borders = (False,10,(0,0,0))\n",
        "\n",
        "## thresholding and morphology\n",
        "# ['try_all_threshold','threshold_otsu','threshold_yen','threshold_isodata','threshold_li','threshold_local','threshold_minimum','threshold_mean', 'threshold_niblack','threshold_sauvola','threshold_triangle','apply_hysteresis_threshold','threshold_multiotsu']\n",
        "thresh_filter = None#skimage.filters.threshold_otsu\n",
        "op_morph = None #apply_morph(img,morph_operation=cv2.MORPH_CLOSE,kernel=(3,3),iterations=1)\n",
        "\n",
        "# all_documents = glob(\"/content/documents/forms/*\")\n",
        "all_documents = glob(\"/content/documents/AI Receipts Project/*\")\n",
        "all_documents.sort()\n",
        "print('Total Documents:',len(all_documents))\n",
        "print(all_documents)\n",
        "\n",
        "!rm -r '/content/all_pdf_tables/'\n",
        "\n",
        "\n",
        "display_info=False\n",
        "save_jpegs = True\n",
        "st_i = None\n",
        "num_forms = 1\n",
        "end_i = None#st_i+num_forms\n",
        "\n",
        "path_write_images = '/content/all_pdf_tables/'\n",
        "if not os.path.isdir(path_write_images):\n",
        "    os.mkdir(path_write_images)\n",
        "\n",
        "all_corrupts = []\n",
        "for doc_index in tqdm(range(0,len(all_documents))):\n",
        "\n",
        "    pdf_path = all_documents[doc_index]\n",
        "    pdf_name,_ = os.path.splitext(pdf_path.split('/')[-1])\n",
        "    if not os.path.isdir(os.path.join(path_write_images,pdf_name)):\n",
        "        os.mkdir(os.path.join(path_write_images,pdf_name))\n",
        "\n",
        "    print(pdf_path,pdf_name)\n",
        "    print('Size(Mb):',round(os.path.getsize(pdf_path)/1024/1024))\n",
        "\n",
        "    ## convert pdf to images\n",
        "    all_pages = convert_from_path(pdf_path)\n",
        "    print('Total Pages:',len(all_pages))\n",
        "\n",
        "\n",
        "    for j in tqdm(range(0,len(all_pages))):\n",
        "        # img = cv2.imread(all_pages[j],0)\n",
        "        img = cv2.cvtColor(np.array(all_pages[j]),cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # try:\n",
        "        #     img = check_orientation_tesseract(np.array(img))\n",
        "        # except:\n",
        "        #     pass\n",
        "\n",
        "        img = preprocess_image(img,deskew=op_deskew,img_thresh=op_img_thresh,\\\n",
        "                                add_border=op_borders[0],border_width=op_borders[1],border_color=op_borders[2],img_type='gray')\n",
        "\n",
        "        img_h,img_w = img.shape\n",
        "        init_type = detect_tables_vs_nontables(img,display_info=display_info)\n",
        "\n",
        "        save_img_name = os.path.join(path_write_images,pdf_name,str(j+1)+'_page_'+str(init_type)+'.jpg')\n",
        "        cv2.imwrite(save_img_name,np.array(all_pages[j]),[cv2.IMWRITE_JPEG_QUALITY, 10])\n",
        "\n",
        "    # zip_name = os.path.join(save_zip_path,pdf_name+'.zip')\n",
        "    # !zip -r {zip_name} {path_write_images}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r '/content/all_pdf_tables.zip' '/content/all_pdf_tables/'\n",
        "!cp '/content/all_pdf_tables.zip' '/content/drive/MyDrive/immediate/'"
      ],
      "metadata": {
        "id": "HlIQfhRfsfmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSLWpFkD8G8X"
      },
      "outputs": [],
      "source": [
        "# print(corrupts)\n",
        "\n",
        "# [('A-2', 1), ('D-1', 3), ('D-1', 6), ('H-1', 31), ('P-2', 44), ('R-1', 39), ('V-1', 15)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAqF9zO0v7ua"
      },
      "outputs": [],
      "source": [
        "# B-2 3 what form is this\n",
        "# K-1 3  what form is this\n",
        "# M-2 62,69 what form is this\n",
        "# P-1 30 ma masla ha. pichlay waloon k saat append hua wa tha\n",
        "# P-2 20 what form is this\n",
        "# R-1 24 what form is this\n",
        "# S-1 19 \"Biometric Notification\"\n",
        "# V-1 3  \"Biometric Notification\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vs5bkdQ2bj2"
      },
      "outputs": [],
      "source": [
        "# junk 5\n",
        "# C-2 45-46-47-48 should have 4 pages\n",
        "# S-1 completely bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UUPpq21qDS"
      },
      "outputs": [],
      "source": [
        "## if name==None, case=485, doc_type=Interview   THEN page=2\n",
        "## if prevous doc_type=RFE and current name,doc_type,case_type=None THEN page=2\n",
        "## if previous=None,None,None and current=None,None,None then page=3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tables vs non tables\n",
        "# B2-3\n",
        "# J2-7\n",
        "# K1-3\n",
        "# M1-19\n",
        "# M2-40\n",
        "# P1-28\n",
        "# P2-20\n",
        "# R1-24\n",
        "# V2-16\n",
        "\n",
        "\n",
        "# F2-18\n",
        "# S1-19"
      ],
      "metadata": {
        "id": "Q6FkKpRAYRzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8QGdlkvHENe"
      },
      "outputs": [],
      "source": [
        "sad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3JGU1epI8LV"
      },
      "source": [
        "# Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cydx-VbyK8oi"
      },
      "outputs": [],
      "source": [
        "# all_documents = glob(\"/content/documents/forms/*\")\n",
        "all_documents = glob(\"/content/documents/AI Receipts Project/*\")\n",
        "all_documents.sort()\n",
        "print('Total Documents:',len(all_documents))\n",
        "print(all_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZMSkxTaK_fJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Settings\n",
        "\n",
        "## pre processing options\n",
        "op_dpi = (300,300)\n",
        "op_deskew = True\n",
        "op_img_thresh = False   ## True makes I to 1 so keep it false\n",
        "op_borders = (False,10,(0,0,0))\n",
        "\n",
        "## thresholding and morphology\n",
        "# ['try_all_threshold','threshold_otsu','threshold_yen','threshold_isodata','threshold_li','threshold_local','threshold_minimum','threshold_mean', 'threshold_niblack','threshold_sauvola','threshold_triangle','apply_hysteresis_threshold','threshold_multiotsu']\n",
        "thresh_filter = None#skimage.filters.threshold_otsu\n",
        "op_morph = None #apply_morph(img,morph_operation=cv2.MORPH_CLOSE,kernel=(3,3),iterations=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXKclLCdvZU9"
      },
      "outputs": [],
      "source": [
        "# path_write_pdf = os.path.join('/content/',pdf_name+'_pdfs/')\n",
        "# path_write_images = os.path.join('/content/',pdf_name+'_images/')\n",
        "\n",
        "# !rm -r {path_write_pdf}\n",
        "# !rm -r {path_write_images}\n",
        "\n",
        "# if not os.path.isdir(path_write_pdf):\n",
        "#     os.mkdir(path_write_pdf)\n",
        "\n",
        "\n",
        "# if not os.path.isdir(path_write_images):\n",
        "#     os.mkdir(path_write_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9O6AtFfzxW8"
      },
      "outputs": [],
      "source": [
        "# !rm -r '/content/drive/MyDrive/micheal_junk_7/'\n",
        "!mkdir '/content/drive/MyDrive/micheal_junk_8/'\n",
        "save_zip_path = '/content/drive/MyDrive/micheal_junk_8/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKbKYr17CJ3Y"
      },
      "outputs": [],
      "source": [
        "display_info=False\n",
        "save_jpegs = True\n",
        "st_i = None\n",
        "num_forms = 1\n",
        "end_i = None#st_i+num_forms\n",
        "\n",
        "all_corrupts = []\n",
        "for doc_index in tqdm(range(0,len(all_documents))):\n",
        "\n",
        "    pdf_path = all_documents[doc_index]\n",
        "    pdf_name,_ = os.path.splitext(pdf_path.split('/')[-1])\n",
        "    print(pdf_path,pdf_name)\n",
        "    print('Size(Mb):',round(os.path.getsize(pdf_path)/1024/1024))\n",
        "\n",
        "    if os.path.isfile(os.path.join(save_zip_path,pdf_name+'.zip')):\n",
        "        continue\n",
        "\n",
        "    ## convert pdf to images\n",
        "    all_pages = convert_from_path(pdf_path)\n",
        "    print('Total Pages:',len(all_pages))\n",
        "\n",
        "    junk_folder = os.path.join('/content/',pdf_name+'_junk/')\n",
        "    !rm -r {junk_folder}\n",
        "    if not os.path.isdir(junk_folder):\n",
        "        os.mkdir(junk_folder)\n",
        "\n",
        "    path_write_pdf = os.path.join('/content/',pdf_name+'_pdfs/')\n",
        "    path_write_images = os.path.join('/content/',pdf_name+'_images/')\n",
        "\n",
        "    !rm -r {path_write_pdf}\n",
        "    !rm -r {path_write_images}\n",
        "\n",
        "    if not os.path.isdir(path_write_pdf):\n",
        "        os.mkdir(path_write_pdf)\n",
        "\n",
        "\n",
        "    if not os.path.isdir(path_write_images):\n",
        "        os.mkdir(path_write_images)\n",
        "\n",
        "    all_final_names,all_final_appends,corrupts = process_and_save(all_pages,junk_folder,display_info=display_info,st_i=st_i,end_i=end_i,save_jpegs=save_jpegs)\n",
        "    all_corrupts.append(all_corrupts)\n",
        "\n",
        "    all_files_pagewise = make_pagewise_list(all_final_names,all_final_appends)\n",
        "    break_pdf_to_files(pdf_path,path_write_pdf,all_files_pagewise,compress_pdf=True)\n",
        "\n",
        "    selected_pages = all_pages[st_i:end_i]\n",
        "    break_pdf_to_images(selected_pages,path_write_images,all_files_pagewise,jpeg_quality=10)\n",
        "\n",
        "    zip_name = os.path.join(save_zip_path,pdf_name+'.zip')\n",
        "    !zip -r {zip_name} {path_write_images}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dknvW1llztnw"
      },
      "outputs": [],
      "source": [
        "print(all_corrupts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL53nBO8NH_n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSSAK5Pkvx8f"
      },
      "outputs": [],
      "source": [
        "# !mkdir '/content/drive/MyDrive/micheal_junk_5/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZrvRUlxWJ8Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(corrupts)\n",
        "\n",
        "# [('A-2', 1), ('D-1', 3), ('D-1', 6), ('H-1', 31), ('P-2', 44), ('R-1', 39), ('V-1', 15)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZGRwP3CI6Of"
      },
      "outputs": [],
      "source": [
        "# !mkdir '/content/drive/MyDrive/micheal_junk_2/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-32DS5Irk1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slXOUsnqti0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4y1Z2JvI6MC"
      },
      "outputs": [],
      "source": [
        "# # all_documents = glob(\"/content/documents/forms/*\")\n",
        "# all_documents = glob(\"/content/documents/AI Receipts Project/*\")\n",
        "# all_documents.sort()\n",
        "# print('Total Documents:',len(all_documents))\n",
        "# print(all_documents)\n",
        "\n",
        "# !rm -r '/content/all_pdf_images/'\n",
        "# display_info=False\n",
        "# save_jpegs = True\n",
        "# st_i = None\n",
        "# num_forms = 1\n",
        "# end_i = None#st_i+num_forms\n",
        "\n",
        "# path_write_images = '/content/all_pdf_images/'\n",
        "# if not os.path.isdir(path_write_images):\n",
        "#     os.mkdir(path_write_images)\n",
        "\n",
        "# all_corrupts = []\n",
        "# for doc_index in tqdm(range(0,len(all_documents))):\n",
        "\n",
        "#     pdf_path = all_documents[doc_index]\n",
        "#     pdf_name,_ = os.path.splitext(pdf_path.split('/')[-1])\n",
        "#     if not os.path.isdir(os.path.join(path_write_images,pdf_name)):\n",
        "#         os.mkdir(os.path.join(path_write_images,pdf_name))\n",
        "\n",
        "#     print(pdf_path,pdf_name)\n",
        "#     print('Size(Mb):',round(os.path.getsize(pdf_path)/1024/1024))\n",
        "\n",
        "#     ## convert pdf to images\n",
        "#     all_pages = convert_from_path(pdf_path)\n",
        "#     print('Total Pages:',len(all_pages))\n",
        "\n",
        "\n",
        "#     for j in range(0,len(all_pages)):\n",
        "#         save_img_name = os.path.join(path_write_images,pdf_name,'page_'+str(j+1)+'.jpg')\n",
        "#         cv2.imwrite(save_img_name,np.array(all_pages[j]),[cv2.IMWRITE_JPEG_QUALITY, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES74m4ZTqRcJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3cHXOUwpc-E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--eU0lgAVfj6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS18UAyNVTMr"
      },
      "source": [
        "# Split document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWacdipXNJJR"
      },
      "outputs": [],
      "source": [
        "# !pip install PyPDF2\n",
        "\n",
        "# !mkdir /content/ddd\n",
        "# from PyPDF2 import PdfWriter, PdfReader\n",
        "\n",
        "# inputpdf = PdfReader(open('/content/documents/forms/Michael Forms.pdf', \"rb\"))\n",
        "\n",
        "# for i in range(len(inputpdf.pages)):\n",
        "#     output = PdfWriter()\n",
        "#     output.add_page(inputpdf.pages[i])\n",
        "#     with open(\"/content/ddd/document-page%s.pdf\" % i, \"wb\") as outputStream:\n",
        "#         output.write(outputStream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCCj0dNfU5nb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lKMUI13U5ks"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA0QGi99vwmp"
      },
      "source": [
        "#Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DuB5SpAjj2J"
      },
      "outputs": [],
      "source": [
        "## Notes\n",
        "# deskew\n",
        "# dpi 300 best\n",
        "# border/no-border\n",
        "\n",
        "# Test\n",
        "# test thresholding\n",
        "# load_system_dawg and load_freq_dawg to false\n",
        "# The text detection for tables is image resolution*2 and for normal document is resolution*1. remember 30pix is ideal case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiX57YPTjjzz"
      },
      "outputs": [],
      "source": [
        "# Ideas\n",
        "# Find just the table or its entries and read the values directly for example the cell with petitioner should have a have directly below it\n",
        "# Delete the unnecessery columns\n",
        "# Read the document only optill a certain point\n",
        "# Put black color on the unnecessery areas\n",
        "# .tiff format for printing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCsHS8vjjxa"
      },
      "outputs": [],
      "source": [
        "## explanation for psm types\n",
        "# yaheen sa manay --psm 4 for tables dakha tha\n",
        "# https://pyimagesearch.com/2021/11/15/tesseract-page-segmentation-modes-psms-explained-how-to-improve-your-ocr-accuracy/\n",
        "\n",
        "\n",
        "## detecting tables from images\n",
        "# https://betterprogramming.pub/extracting-tables-from-images-in-python-made-easy-ier-3be959555f6f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2aV41dxjjvA"
      },
      "outputs": [],
      "source": [
        "## for table detection\n",
        "'''\n",
        "Note: Depending on the image, you may have to modify the kernel size. For instance to capture longer horizontal lines,\n",
        "it may be necessary to increase the horizontal kernel from (40, 1) to say (80, 1). If you wanted to detect thicker horizontal lines,\n",
        "then you could increase the width of the kernel to say (80, 2). In addition, you could increase the number of iterations when performing cv2.morphologyEx().\n",
        "Similarly, you could modify the vertical kernels to detect more or less vertical lines.\n",
        "There is a trade-off when increasing or decreasing the kernel size as you may capture more or less of the lines. Again, it all varies depending on the input image\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU59mrfFjjse"
      },
      "outputs": [],
      "source": [
        "## For case type detection\n",
        "'''\n",
        "make sure if all case types are I-something. If thats the case the just hadcode I and then extract the rest of it.\n",
        "some case dont have a \"-\" in I-123  see how to tackle that?\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lx8MQVRNQDJ"
      },
      "outputs": [],
      "source": [
        "## utilize str.strip() and str.replace()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c1vtJk4svpX3",
        "1rPtfBIaJ70x",
        "9pVhgkbH9OV7",
        "r4p63V3D6KuL",
        "gKKU9d6gNBAE",
        "upLSWXDX9VI0",
        "6GSuuNiiJxjJ",
        "sl2FDuGk9pQE",
        "O3JGU1epI8LV",
        "jS18UAyNVTMr"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}